{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python368jvsc74a57bd0619141f40d6660e90435fdea452ff8c8a3cfa2ce2d42e30ecc199b88a9b310ca",
   "display_name": "Python 3.6.8 64-bit ('env_predictSF')"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Analyse des scores de risque Signaux Faibles - Création d'indicateurs régionaux\n",
    "Dans le cadre d'une demande du Ministère du Travail, Signaux Faibles réalise une analyse agrégée aux niveaux géographiques de la région et du département, pour fournir différents indicateurs de risque territorialisés.\n",
    "\n",
    "Ce notebook vise à charger les données provenant de nos prédictions de risque pour Mars 2020, et à produire des indicateurs agrégés qui seront ultérieurement aposés sur des fonds de cartographie."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from pymongo import MongoClient\n",
    "from pymongo.cursor import Cursor\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "# Set logging level to INFO\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "from predictsignauxfaibles.utils import MongoDBQuery, MongoParams\n",
    "import predictsignauxfaibles.config as global_config\n",
    "\n",
    "import config as cab_config\n",
    "import utils\n"
   ]
  },
  {
   "source": [
    "# Part 1 - Métriques à mars 2020 (modèle)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_features_from_file = True\n",
    "load_scores_from_file = True\n",
    "features_path = \"/home/simon.lebastard/predictsignauxfaibles/data/features_2103.json\"\n",
    "scores_path = \"/home/simon.lebastard/predictsignauxfaibles/data/scores_2103.json\"\n",
    "postproc_path = \"/home/simon.lebastard/predictsignauxfaibles/data/postproc_2103.json\""
   ]
  },
  {
   "source": [
    "## Fetching features and scores data\n",
    "### Option 1: Fetch data from each dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "features = utils.load_features(date_min=\"2020-02-01\", date_max=\"2020-02-28\", from_file=True, filepath=features_path)\n",
    "logging.info(f\"Loaded {features.shape[0]} rows and {features.shape[1]} columns\")"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:root:Succesfully loaded Features data from /home/simon.lebastard/predictsignauxfaibles/data/features_2103.json\n",
      "INFO:root:Loaded 956765 rows and 31 columns\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:root:Succesfully loaded Scores data from /home/simon.lebastard/predictsignauxfaibles/data/scores_2103.json\n",
      "INFO:root:Loaded 657296 rows and 7 columns\n"
     ]
    }
   ],
   "source": [
    "scores = utils.load_scores(batch_name=\"2102_altares\", algo_name=\"mars2021_v0\", from_file=True, filepath=scores_path)\n",
    "logging.info(f\"Loaded {scores.shape[0]} rows and {scores.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[\"periode\"] = features.periode.apply(utils.datetime_to_str)\n",
    "scores[\"periode\"] = scores.periode.apply(utils.datetime_to_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(scores, features, on=['siret', 'periode'], how='inner')\n",
    "df[\"code_reg\"] = df.region.apply(utils.map_region_to_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile(postproc_path):\n",
    "    logging.info(\"Saving joined post-processed data to disk...\")\n",
    "    df.to_json(postproc_path, orient=\"records\", default_handler=str)\n",
    "    logging.info(f\"Saved to {postproc_path}\")"
   ]
  },
  {
   "source": [
    "### Option 2: Load data directly from df stored on disk"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(postproc_path):\n",
    "    print(\"Loading post-processed data to disk\")\n",
    "    df = pd.read_json(postproc_path, orient=\"records\")"
   ]
  },
  {
   "source": [
    "## Aggregation of region-wide features\n",
    "\n",
    "Niveaux de granularité considérés:\n",
    "- région\n",
    "- département\n",
    "\n",
    "Pour chaque niveau de granularité:\n",
    "- compter le nombre d'établissements flaguées rouge par région\n",
    "- compter le nombre d'établissements flaguées orange par région\n",
    "- compter le nombre d'établissements flaguées en rouge OU en orange\n",
    "- rapporter ces nombre d'établissements au nombre total d'établissements dans la zone géographique\n",
    "- compter le nombre de défaillances effectives sur une période donnée, et calculer le ratio correspondants\n",
    "\n",
    "Pour toutes les grandeurs calculées précédemment, calculer des équivalents en nombre d'employés concernés.\n",
    "\n",
    "Pour les établissements ayant un risque de défaillance modéré ou fort (flagguée en rouge OU en orange), communiquer:\n",
    "- ratio $\\frac{dette_{ouvriere}}{cotisation}$ moyen\n",
    "- recours moyen à l'activité partielle\n",
    "\n",
    "On pourra éventuellement ajouter à ces premières métriques:\n",
    "- des ratios financiers provenant de la DE de la Banque de France\n",
    "- le nombre de jour moyen de retard de paiement aux fournisseurs (donnée Paydex)\n",
    "\n",
    "### Preprocessing steps"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    # create an outcome flag based only on failures since the beginning of the COVID crisis\n",
    "    df[\"failure\"] = (df[\"time_til_failure\"]>=0) & (df[\"time_til_failure\"]<12) # todo: automatiser le nombre de mois à regarder vers l'avant: entre mars 2020 et <THIS_MONTH>\n",
    "    df[\"failure\"] = df.failure.astype(int)\n",
    "\n",
    "    # encode alert level into integer\n",
    "    df[\"alert_flag\"] = df.alert.replace({\"Pas d'alerte\": 0, \"Alerte seuil F1\": 1, \"Alerte seuil F2\": 2})\n",
    "    df[\"alert_bin\"] = (df.alert_flag > 0)\n",
    "\n",
    "    # ratio dette/cotisation sur la part salariale des cotisations sociales\n",
    "    df[\"ratio_dette_ouvriere\"] = df[\"montant_part_ouvriere\"] / df[\"cotisation\"]\n",
    "    df[\"ratio_dette_patronale\"] = df[\"montant_part_patronale\"] / df[\"cotisation\"]\n",
    "    return df\n",
    "\n",
    "def replace_nans(df, replace_dct):\n",
    "    for field, rpl in replace_dct.items():\n",
    "        df[field].fillna(value=rpl, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_nans(df, cab_config.NAN_RPL)\n",
    "df = preprocess(df)"
   ]
  },
  {
   "source": [
    "Vérification des effectifs par catégorie:\n",
    "- entrée en procédure collective\n",
    "- flagging par l'algorithme SF\n",
    "- flagging binaire par l'algorithme SF (True si un établissement est flagué en rouge OU en orange, False si l'établissement est flagué Vert, ie non flagué)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Building aggregation dataframe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cab_config.FEATURES_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_stats(geo_attr: str, outcome_attr: str, siret_per_ape3_min: int=10, siret_per_naf_min: int=25):\n",
    "        assert outcome_attr in [\"alert_flag\", \"alert_bin\", \"failure\", \"outcome\"]\n",
    "                \n",
    "        risk_ape3_stats = df.groupby(by=[geo_attr,outcome_attr,\"code_naf\",\"libelle_naf\",\"code_ape_niveau3\",\"libelle_ape3\"]).agg(\n",
    "                siret_count=('siret', 'count'),\n",
    "                effectif_tot=('effectif', 'sum'),\n",
    "        )\n",
    "        risk_naf_stats = df.groupby(by=[geo_attr,outcome_attr,\"code_naf\",\"libelle_naf\"]).agg(\n",
    "                siret_count=('siret', 'count'),\n",
    "                effectif_tot=('effectif', 'sum')\n",
    "        )\n",
    "        risk_stats = df.groupby(by=[geo_attr,outcome_attr]).agg(\n",
    "            siret_count=('siret', 'count'),\n",
    "            effectif_tot=('effectif', 'sum'),\n",
    "            ratiodette_ouvr_avg=('ratio_dette_ouvriere', 'mean'),\n",
    "            ratiodette_patr_avg=('ratio_dette_patronale', 'mean'),\n",
    "            ratiodette_avg=('ratio_dette', 'mean'),\n",
    "            apart_autr_avg=('apart_heures_autorisees', 'mean'),\n",
    "            apart_cons_avg=('apart_heures_consommees', 'mean'),\n",
    "            apart_cumcons_avg=('apart_heures_consommees_cumulees', 'mean'),\n",
    "            paydex_avg=('paydex_nb_jours', 'mean'),\n",
    "            taux_endettement_avg=('taux_endettement', 'mean'),\n",
    "        )\n",
    "        national_stats = df.groupby(by=[outcome_attr]).agg(\n",
    "            siret_count=('siret', 'count'),\n",
    "            effectif_tot=('effectif', 'sum'),\n",
    "            ratiodette_ouvr_avg=('ratio_dette_ouvriere', 'mean'),\n",
    "            ratiodette_patr_avg=('ratio_dette_patronale', 'mean'),\n",
    "            ratiodette_avg=('ratio_dette', 'mean'),\n",
    "            apart_autr_avg=('apart_heures_autorisees', 'mean'),\n",
    "            apart_cons_avg=('apart_heures_consommees', 'mean'),\n",
    "            apart_cumcons_avg=('apart_heures_consommees_cumulees', 'mean'),\n",
    "            paydex_avg=('paydex_nb_jours', 'mean'),\n",
    "            taux_endettement_avg=('taux_endettement', 'mean'),\n",
    "        )\n",
    "        \n",
    "        # Calcul du ratio des effectifs/établissements par rapport au total de la zone géographique\n",
    "        risk_stats['siret_rate'] = risk_stats.siret_count / risk_stats.groupby(by=geo_attr).siret_count.sum()\n",
    "        national_stats['siret_rate'] = national_stats.siret_count / national_stats.siret_count.sum()\n",
    "        \n",
    "        risk_stats['effectif_rate'] = risk_stats.siret_count / risk_stats.groupby(by=geo_attr).effectif_tot.sum()\n",
    "        national_stats['effectif_rate'] = national_stats.effectif_tot / national_stats.effectif_tot.sum()\n",
    "        \n",
    "        ####################\n",
    "        ### ANALYSES SECTORIELLES\n",
    "        ####################\n",
    "\n",
    "        risk_ape3_base = risk_ape3_stats.loc[risk_ape3_stats.index.get_level_values(\"code_naf\").isin(cab_config.NAF_INDUSTRY)]\n",
    "        risk_ape3_base = risk_ape3_base[risk_ape3_base[\"siret_count\"] >= siret_per_ape3_min]\n",
    "        risk_naf_base = risk_naf_stats.loc[risk_naf_stats.index.get_level_values(\"code_naf\").isin(cab_config.NAF_INDUSTRY)]\n",
    "        risk_naf_base = risk_naf_base[risk_naf_base[\"siret_count\"] >= siret_per_naf_min]\n",
    "\n",
    "        #############\n",
    "        ## Analyses sectorielles - Nombre d'employés\n",
    "        #############\n",
    "        # APE level 3\n",
    "        risk_stats['ape3_mostatrisk_eff_abs'] = risk_ape3_base.groupby(by=[geo_attr,outcome_attr]).effectif_tot.idxmax()\n",
    "        risk_stats['ape3_mostatrisk_eff_abs_tot'] = risk_ape3_base.groupby(by=[geo_attr,outcome_attr]).effectif_tot.max()\n",
    "        risk_stats['ape3_mostatrisk_eff_abs_code'] = risk_stats['ape3_mostatrisk_eff_abs'].apply(lambda x: x[4] if isinstance(x, tuple) else \"None\")\n",
    "        risk_stats['ape3_mostatrisk_eff_abs_libelle'] = risk_stats['ape3_mostatrisk_eff_abs'].apply(lambda x: x[5] if isinstance(x, tuple) else \"None\")\n",
    "        risk_stats.drop(['ape3_mostatrisk_eff_abs'], axis=1, inplace=True)\n",
    "        \n",
    "        ape3_risk_overrepresentation_eff = risk_ape3_base.effectif_tot.div(risk_ape3_base.groupby(by=[geo_attr,\"code_naf\",\"libelle_naf\",\"code_ape_niveau3\",\"libelle_ape3\"]).agg({\"effectif_tot\": 'sum'}).effectif_tot, axis=0)\n",
    "        risk_stats['ape3_risk_eff_most_overrepresented'] = ape3_risk_overrepresentation_eff.groupby(by=[geo_attr,outcome_attr]).idxmax()\n",
    "        risk_stats['ape3_risk_eff_most_overrepresented_rate'] = ape3_risk_overrepresentation_eff.groupby(by=[geo_attr,outcome_attr]).max()\n",
    "        risk_stats['ape3_risk_eff_most_overrepresented_code'] = risk_stats['ape3_risk_eff_most_overrepresented'].apply(lambda x: x[3] if isinstance(x, tuple) else \"None\")\n",
    "        risk_stats['ape3_risk_eff_most_overrepresented_libelle'] = risk_stats['ape3_risk_eff_most_overrepresented'].apply(lambda x: x[4] if isinstance(x, tuple) else \"None\")\n",
    "        risk_stats.drop(['ape3_risk_eff_most_overrepresented'], axis=1, inplace=True)\n",
    "        \n",
    "        for naf in cab_config.NAF_INDUSTRY:\n",
    "            risk_stats[f\"ape3_mostatrisk_eff_abs_{naf}\"] = risk_ape3_base.loc[risk_ape3_base.index.get_level_values(\"code_naf\")==naf].groupby(by=[geo_attr,outcome_attr]).effectif_tot.idxmax()\n",
    "            risk_stats[f\"ape3_mostatrisk_eff_abs_{naf}_tot\"] = risk_ape3_base.loc[risk_ape3_base.index.get_level_values(\"code_naf\").isin(cab_config.NAF_INDUSTRY)].groupby(by=[geo_attr,outcome_attr]).effectif_tot.max()\n",
    "            risk_stats[f\"ape3_mostatrisk_eff_abs_{naf}_code\"] = risk_stats[f\"ape3_mostatrisk_eff_abs_{naf}\"].apply(lambda x: x[4] if isinstance(x, tuple) else \"None\")\n",
    "            risk_stats[f\"ape3_mostatrisk_eff_abs_{naf}_libelle\"] = risk_stats[f\"ape3_mostatrisk_eff_abs_{naf}\"].apply(lambda x: x[5] if isinstance(x, tuple) else \"None\")\n",
    "            risk_stats.drop([f\"ape3_mostatrisk_eff_abs_{naf}\"], axis=1, inplace=True)\n",
    "        \n",
    "            ape3_risk_overrepresentation_eff = risk_ape3_base.loc[risk_ape3_base.index.get_level_values(\"code_naf\")==naf].effectif_tot.div(risk_ape3_base.loc[risk_ape3_base.index.get_level_values(\"code_naf\")==naf].groupby(by=[geo_attr,\"code_naf\",\"libelle_naf\",\"code_ape_niveau3\",\"libelle_ape3\"]).agg({\"effectif_tot\": 'sum'}).effectif_tot, axis=0)\n",
    "            risk_stats[f\"ape3_risk_eff_most_overrepresented_{naf}\"] = ape3_risk_overrepresentation_eff.groupby(by=[geo_attr,outcome_attr]).idxmax()\n",
    "            risk_stats[f\"ape3_risk_eff_most_overrepresented_{naf}_rate\"] = ape3_risk_overrepresentation_eff.groupby(by=[geo_attr,outcome_attr]).max()\n",
    "            risk_stats[f\"ape3_risk_eff_most_overrepresented_{naf}_code\"] = risk_stats[f\"ape3_risk_eff_most_overrepresented_{naf}\"].apply(lambda x: x[3] if isinstance(x, tuple) else \"None\")\n",
    "            risk_stats[f\"ape3_risk_eff_most_overrepresented_{naf}_libelle\"] = risk_stats[f\"ape3_risk_eff_most_overrepresented_{naf}\"].apply(lambda x: x[4] if isinstance(x, tuple) else \"None\")\n",
    "            risk_stats.drop([f\"ape3_risk_eff_most_overrepresented_{naf}\"], axis=1, inplace=True)\n",
    "        \n",
    "        # NAF\n",
    "        risk_stats['naf_mostatrisk_eff_abs'] = risk_naf_base.groupby(by=[geo_attr,outcome_attr]).effectif_tot.idxmax()\n",
    "        risk_stats['naf_mostatrisk_eff_abs_tot'] = risk_naf_base.groupby(by=[geo_attr,outcome_attr]).effectif_tot.max()\n",
    "        risk_stats['naf_mostatrisk_eff_abs_code'] = risk_stats.naf_mostatrisk_eff_abs.apply(lambda x: x[2] if isinstance(x, tuple) else \"None\")\n",
    "        risk_stats['naf_mostatrisk_eff_abs_libelle'] = risk_stats.naf_mostatrisk_eff_abs.apply(lambda x: x[3] if isinstance(x, tuple) else \"None\")\n",
    "        risk_stats.drop(['naf_mostatrisk_eff_abs'], axis=1, inplace=True)\n",
    "        \n",
    "        naf_risk_overrepresentation_eff = risk_naf_base.effectif_tot.div(risk_naf_base.groupby(by=[geo_attr,\"code_naf\",\"libelle_naf\"]).agg({\"effectif_tot\": 'sum'}).effectif_tot, axis=0)\n",
    "        risk_stats['naf_risk_eff_most_overrepresented'] = naf_risk_overrepresentation_eff.groupby(by=[geo_attr,outcome_attr]).idxmax()\n",
    "        risk_stats['naf_risk_eff_most_overrepresented_rate'] = naf_risk_overrepresentation_eff.groupby(by=[geo_attr,outcome_attr]).max()\n",
    "        risk_stats['naf_risk_eff_most_overrepresented_code'] = risk_stats['naf_risk_eff_most_overrepresented'].apply(lambda x: x[1] if isinstance(x, tuple) else \"None\")\n",
    "        risk_stats['naf_risk_eff_most_overrepresented_libelle'] = risk_stats['naf_risk_eff_most_overrepresented'].apply(lambda x: x[2] if isinstance(x, tuple) else \"None\")\n",
    "        risk_stats.drop(['naf_risk_eff_most_overrepresented'], axis=1, inplace=True)\n",
    "        \n",
    "        #############\n",
    "        ## Analyses sectorielles - Nombre d'établissements\n",
    "        #############\n",
    "        # APE level 3\n",
    "        risk_stats['ape3_mostatrisk_etab_abs'] = risk_ape3_base.groupby(by=[geo_attr,outcome_attr]).siret_count.idxmax()\n",
    "        risk_stats['ape3_mostatrisk_etab_abs_tot'] = risk_ape3_base.groupby(by=[geo_attr,outcome_attr]).siret_count.max()\n",
    "        risk_stats['ape3_mostatrisk_etab_abs_code'] = risk_stats['ape3_mostatrisk_etab_abs'].apply(lambda x: x[4] if isinstance(x, tuple) else \"None\")\n",
    "        risk_stats['ape3_mostatrisk_etab_abs_libelle'] = risk_stats['ape3_mostatrisk_etab_abs'].apply(lambda x: x[5] if isinstance(x, tuple) else \"None\")\n",
    "        risk_stats.drop(['ape3_mostatrisk_etab_abs'], axis=1, inplace=True)\n",
    "        \n",
    "        ape3_risk_overrepresentation_etab = risk_ape3_base.siret_count.div(risk_ape3_base.groupby(by=[geo_attr,\"code_naf\",\"libelle_naf\",\"code_ape_niveau3\",\"libelle_ape3\"]).agg({\"siret_count\": 'sum'}).siret_count, axis=0)\n",
    "        risk_stats['ape3_risk_etab_most_overrepresented'] = ape3_risk_overrepresentation_etab.groupby(by=[geo_attr,outcome_attr]).idxmax()\n",
    "        risk_stats['ape3_risk_etab_most_overrepresented_rate'] = ape3_risk_overrepresentation_etab.groupby(by=[geo_attr,outcome_attr]).max()\n",
    "        risk_stats['ape3_risk_etab_most_overrepresented_code'] = risk_stats['ape3_risk_etab_most_overrepresented'].apply(lambda x: x[3] if isinstance(x, tuple) else \"None\")\n",
    "        risk_stats['ape3_risk_etab_most_overrepresented_libelle'] = risk_stats['ape3_risk_etab_most_overrepresented'].apply(lambda x: x[4] if isinstance(x, tuple) else \"None\")\n",
    "        risk_stats.drop(['ape3_risk_etab_most_overrepresented'], axis=1, inplace=True)\n",
    "        \n",
    "        for naf in cab_config.NAF_INDUSTRY:\n",
    "            risk_stats[f\"ape3_mostatrisk_etab_abs_{naf}\"] = risk_ape3_base.loc[risk_ape3_base.index.get_level_values(\"code_naf\")==naf].groupby(by=[geo_attr,outcome_attr]).siret_count.idxmax()\n",
    "            risk_stats[f\"ape3_mostatrisk_etab_abs_{naf}_tot\"] = risk_ape3_base.loc[risk_ape3_base.index.get_level_values(\"code_naf\").isin(cab_config.NAF_INDUSTRY)].groupby(by=[geo_attr,outcome_attr]).siret_count.max()\n",
    "            risk_stats[f\"ape3_mostatrisk_etab_abs_{naf}_code\"] = risk_stats[f\"ape3_mostatrisk_etab_abs_{naf}\"].apply(lambda x: x[4] if isinstance(x, tuple) else \"None\")\n",
    "            risk_stats[f\"ape3_mostatrisk_etab_abs_{naf}_libelle\"] = risk_stats[f\"ape3_mostatrisk_etab_abs_{naf}\"].apply(lambda x: x[5] if isinstance(x, tuple) else \"None\")\n",
    "            risk_stats.drop([f\"ape3_mostatrisk_etab_abs_{naf}\"], axis=1, inplace=True)\n",
    "        \n",
    "            ape3_risk_overrepresentation_etab = risk_ape3_base.loc[risk_ape3_base.index.get_level_values(\"code_naf\")==naf].siret_count.div(risk_ape3_base.loc[risk_ape3_base.index.get_level_values(\"code_naf\")==naf].groupby(by=[geo_attr,\"code_naf\",\"libelle_naf\",\"code_ape_niveau3\",\"libelle_ape3\"]).agg({\"siret_count\": 'sum'}).siret_count, axis=0)\n",
    "            risk_stats[f\"ape3_risk_etab_most_overrepresented_{naf}\"] = ape3_risk_overrepresentation_etab.groupby(by=[geo_attr,outcome_attr]).idxmax()\n",
    "            risk_stats[f\"ape3_risk_etab_most_overrepresented_{naf}_rate\"] = ape3_risk_overrepresentation_etab.groupby(by=[geo_attr,outcome_attr]).max()\n",
    "            risk_stats[f\"ape3_risk_etab_most_overrepresented_{naf}_code\"] = risk_stats[f\"ape3_risk_etab_most_overrepresented_{naf}\"].apply(lambda x: x[3] if isinstance(x, tuple) else \"None\")\n",
    "            risk_stats[f\"ape3_risk_etab_most_overrepresented_{naf}_libelle\"] = risk_stats[f\"ape3_risk_etab_most_overrepresented_{naf}\"].apply(lambda x: x[4] if isinstance(x, tuple) else \"None\")\n",
    "            risk_stats.drop([f\"ape3_risk_etab_most_overrepresented_{naf}\"], axis=1, inplace=True)\n",
    "        \n",
    "        # NAF\n",
    "        risk_stats['naf_mostatrisk_etab_abs'] = risk_naf_base.groupby(by=[geo_attr,outcome_attr]).siret_count.idxmax()\n",
    "        risk_stats['naf_mostatrisk_etab_abs_tot'] = risk_naf_base.groupby(by=[geo_attr,outcome_attr]).siret_count.max()\n",
    "        risk_stats['naf_mostatrisk_etab_abs_code'] = risk_stats.naf_mostatrisk_etab_abs.apply(lambda x: x[2] if isinstance(x, tuple) else \"None\")\n",
    "        risk_stats['naf_mostatrisk_etab_abs_libelle'] = risk_stats.naf_mostatrisk_etab_abs.apply(lambda x: x[3] if isinstance(x, tuple) else \"None\")\n",
    "        risk_stats.drop(['naf_mostatrisk_etab_abs'], axis=1, inplace=True)\n",
    "        \n",
    "        naf_risk_overrepresentation_etab = risk_naf_base.siret_count.div(risk_naf_base.groupby(by=[geo_attr,\"code_naf\",\"libelle_naf\"]).agg({\"siret_count\": 'sum'}).siret_count, axis=0)\n",
    "        risk_stats['naf_risk_etab_most_overrepresented'] = naf_risk_overrepresentation_etab.groupby(by=[geo_attr,outcome_attr]).idxmax()\n",
    "        risk_stats['naf_risk_etab_most_overrepresented_rate'] = naf_risk_overrepresentation_etab.groupby(by=[geo_attr,outcome_attr]).max()\n",
    "        risk_stats['naf_risk_etab_most_overrepresented_code'] = risk_stats['naf_risk_etab_most_overrepresented'].apply(lambda x: x[1] if isinstance(x, tuple) else \"None\")\n",
    "        risk_stats['naf_risk_etab_most_overrepresented_libelle'] = risk_stats['naf_risk_etab_most_overrepresented'].apply(lambda x: x[2] if isinstance(x, tuple) else \"None\")\n",
    "        risk_stats.drop(['naf_risk_etab_most_overrepresented'], axis=1, inplace=True)\n",
    "                \n",
    "        # Rapport des indicateurs calculés à l'échelle nationale: on calcule la sur/sous-représentativité par rapport à la France entière\n",
    "        risk_stats['siret_rate_to_ntl_avg_to_ntl_avg'] = (risk_stats.siret_rate - national_stats.siret_rate) / national_stats.siret_rate\n",
    "        risk_stats['effectif_rate_to_ntl_avg_to_ntl_avg'] = (risk_stats.effectif_rate - national_stats.effectif_rate) / national_stats.effectif_rate\n",
    "        risk_stats['ratiodette_ouvr_avg_to_ntl_avg'] = (risk_stats.ratiodette_ouvr_avg - national_stats.ratiodette_ouvr_avg) / national_stats.ratiodette_ouvr_avg\n",
    "        risk_stats['ratiodette_patr_avg_to_ntl_avg'] = (risk_stats.ratiodette_patr_avg - national_stats.ratiodette_patr_avg) / national_stats.ratiodette_patr_avg\n",
    "        risk_stats['ratiodette_avg_to_ntl_avg'] = (risk_stats.ratiodette_avg - national_stats.ratiodette_avg) / national_stats.ratiodette_avg\n",
    "        risk_stats['apart_autr_avg_to_ntl_avg'] = (risk_stats.apart_autr_avg - national_stats.apart_autr_avg) / national_stats.apart_autr_avg\n",
    "        risk_stats['apart_cons_avg_to_ntl_avg'] = (risk_stats.apart_cons_avg - national_stats.apart_cons_avg) / national_stats.apart_cons_avg\n",
    "        risk_stats['apart_cumcons_avg_to_ntl_avg'] = (risk_stats.apart_cumcons_avg - national_stats.apart_cumcons_avg) / national_stats.apart_cumcons_avg\n",
    "        risk_stats['paydex_avg_to_ntl_avg'] = (risk_stats.paydex_avg - national_stats.paydex_avg) / national_stats.paydex_avg\n",
    "        risk_stats['taux_endettement_avg_to_ntl_avg'] = (risk_stats.taux_endettement_avg - national_stats.taux_endettement_avg) / national_stats.taux_endettement_avg\n",
    "                \n",
    "        return risk_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_risk_stats = aggregate_stats(geo_attr=\"region\", outcome_attr=\"alert_flag\", siret_per_ape3_min=0, siret_per_naf_min=0)\n",
    "reg_riskbin_stats = aggregate_stats(geo_attr=\"region\", outcome_attr=\"alert_bin\", siret_per_ape3_min=0, siret_per_naf_min=0)\n",
    "reg_fail_stats = aggregate_stats(geo_attr=\"region\", outcome_attr=\"failure\", siret_per_ape3_min=0, siret_per_naf_min=0)\n",
    "\n",
    "dpt_risk_stats = aggregate_stats(geo_attr=\"departement\", outcome_attr=\"alert_flag\", siret_per_ape3_min=0, siret_per_naf_min=0)\n",
    "dpt_riskbin_stats = aggregate_stats(geo_attr=\"departement\", outcome_attr=\"alert_bin\", siret_per_ape3_min=0, siret_per_naf_min=0)\n",
    "dpt_fail_stats = aggregate_stats(geo_attr=\"departement\", outcome_attr=\"failure\", siret_per_ape3_min=0, siret_per_naf_min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_risque_rouge = reg_risk_stats[reg_risk_stats.index.get_level_values(\"alert_flag\")==2]\n",
    "reg_risque_orange = reg_risk_stats[reg_risk_stats.index.get_level_values(\"alert_flag\")==1]\n",
    "reg_risque_orange_ou_rouge = reg_riskbin_stats[reg_riskbin_stats.index.get_level_values(\"alert_bin\")==True]\n",
    "reg_risque_vert = reg_riskbin_stats[reg_riskbin_stats.index.get_level_values(\"alert_bin\")==False]\n",
    "reg_fail = reg_fail_stats[reg_fail_stats.index.get_level_values(\"failure\")==True]\n",
    "reg_nofail = reg_fail_stats[reg_fail_stats.index.get_level_values(\"failure\")==False]\n",
    "\n",
    "dpt_risque_rouge = dpt_risk_stats[dpt_risk_stats.index.get_level_values(\"alert_flag\")==2]\n",
    "dpt_risque_orange = dpt_risk_stats[dpt_risk_stats.index.get_level_values(\"alert_flag\")==1]\n",
    "dpt_risque_orange_ou_rouge = dpt_riskbin_stats[dpt_riskbin_stats.index.get_level_values(\"alert_bin\")==True]\n",
    "dpt_risque_vert = dpt_riskbin_stats[dpt_riskbin_stats.index.get_level_values(\"alert_bin\")==False]\n",
    "dpt_fail = dpt_fail_stats[dpt_fail_stats.index.get_level_values(\"failure\")==True]\n",
    "dpt_nofail = dpt_fail_stats[dpt_fail_stats.index.get_level_values(\"failure\")==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_risk_outpath_root = \"/home/simon.lebastard/predictsignauxfaibles/predictsignauxfaibles/notebooks/exports/reg_2103\"\n",
    "dpt_risk_outpath_root = \"/home/simon.lebastard/predictsignauxfaibles/predictsignauxfaibles/notebooks/exports/dpt_2103\"\n",
    "\n",
    "reg_risk_stats.to_csv(f\"{reg_risk_outpath_root}_riskflag_all.csv\")\n",
    "dpt_risk_stats.to_csv(f\"{dpt_risk_outpath_root}_riskflag_all.csv\")\n",
    "reg_riskbin_stats.to_csv(f\"{reg_risk_outpath_root}_riskbin_all.csv\")\n",
    "dpt_riskbin_stats.to_csv(f\"{dpt_risk_outpath_root}_riskbin_all.csv\")\n",
    "reg_fail_stats.to_csv(f\"{reg_risk_outpath_root}_failures_all.csv\")\n",
    "dpt_fail_stats.to_csv(f\"{dpt_risk_outpath_root}_failures_all.csv\")\n",
    "\n",
    "reg_risque_rouge.to_csv(f\"{reg_risk_outpath_root}_risque_rouge.csv\")\n",
    "reg_risque_orange.to_csv(f\"{reg_risk_outpath_root}_risque_orange.csv\")\n",
    "reg_risque_orange_ou_rouge.to_csv(f\"{reg_risk_outpath_root}_risque_orange_ou_rouge.csv\")\n",
    "reg_risque_vert.to_csv(f\"{reg_risk_outpath_root}_risque_vert.csv\")\n",
    "reg_fail.to_csv(f\"{reg_risk_outpath_root}_defaillance.csv\")\n",
    "reg_nofail.to_csv(f\"{reg_risk_outpath_root}_sans_defaillance.csv\")\n",
    "\n",
    "dpt_risque_rouge.to_csv(f\"{dpt_risk_outpath_root}_risque_rouge.csv\")\n",
    "dpt_risque_orange.to_csv(f\"{dpt_risk_outpath_root}_risque_orange.csv\")\n",
    "dpt_risque_orange_ou_rouge.to_csv(f\"{dpt_risk_outpath_root}_risque_orange_ou_rouge.csv\")\n",
    "dpt_risque_vert.to_csv(f\"{dpt_risk_outpath_root}_risque_vert.csv\")\n",
    "dpt_fail.to_csv(f\"{dpt_risk_outpath_root}_defaillance.csv\")\n",
    "dpt_nofail.to_csv(f\"{dpt_risk_outpath_root}_sans_defaillance.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}