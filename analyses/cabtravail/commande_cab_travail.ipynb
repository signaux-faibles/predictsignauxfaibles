{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python368jvsc74a57bd0619141f40d6660e90435fdea452ff8c8a3cfa2ce2d42e30ecc199b88a9b310ca",
   "display_name": "Python 3.6.8 64-bit ('env_predictSF')"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Analyse des scores de risque Signaux Faibles - Création d'indicateurs régionaux\n",
    "Dans le cadre d'une demande du Ministère du Travail, Signaux Faibles réalise une analyse agrégée aux niveaux géographiques de la région et du département, pour fournir différents indicateurs de risque territorialisés.\n",
    "\n",
    "Ce notebook vise à charger les données provenant de nos prédictions de risque pour Mars 2020, et à produire des indicateurs agrégés qui seront ultérieurement aposés sur des fonds de cartographie."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from pymongo import MongoClient\n",
    "from pymongo.cursor import Cursor\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "\n",
    "# Set logging level to INFO\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "from predictsignauxfaibles.utils import MongoDBQuery, MongoParams\n",
    "import predictsignauxfaibles.config as global_config\n",
    "\n",
    "import config as cab_config\n",
    "import utils\n"
   ]
  },
  {
   "source": [
    "# Part 1 - Métriques à mars 2020 (modèle)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_features_from_file = True\n",
    "load_scores_from_file = True\n",
    "features_path = \"/home/simon.lebastard/predictsignauxfaibles/data/features_2103.json\"\n",
    "scores_path = \"/home/simon.lebastard/predictsignauxfaibles/data/scores_2103.json\"\n",
    "postproc_path = \"/home/simon.lebastard/predictsignauxfaibles/data/postproc_2103.json\""
   ]
  },
  {
   "source": [
    "## Fetching features and scores data\n",
    "### Option 1: Fetch data from each dataset"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "features = utils.load_features(date_min=\"2020-02-01\", date_max=\"2020-02-28\", from_file=False, filepath=features_path)\n",
    "logging.info(f\"Loaded {features.shape[0]} rows and {features.shape[1]} columns\")"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:root:Loading features from MongoDB...\n",
      "INFO:root:... Success! Unravelling dataframe...\n",
      "INFO:root:...done!\n",
      "INFO:root:Saving fetched Features data to disk\n",
      "INFO:root:Success\n",
      "INFO:root:Loaded 956765 rows and 31 columns\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:root:Succesfully loaded Scores data from /home/simon.lebastard/predictsignauxfaibles/data/scores_2103.json\n",
      "INFO:root:Loaded 657296 rows and 7 columns\n"
     ]
    }
   ],
   "source": [
    "scores = utils.load_scores(batch_name=\"2102_altares\", algo_name=\"mars2021_v0\", from_file=True, filepath=scores_path)\n",
    "logging.info(f\"Loaded {scores.shape[0]} rows and {scores.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[\"periode\"] = features.periode.apply(datetime_to_str)\n",
    "scores[\"periode\"] = scores.periode.apply(datetime_to_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(scores, features, on=['siret', 'periode'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "INFO:root:Saving joined post-processed data to disk...\n",
      "INFO:root:Saved to /home/simon.lebastard/predictsignauxfaibles/data/postproc_2103.json\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isfile(postproc_path):\n",
    "    logging.info(\"Saving joined post-processed data to disk...\")\n",
    "    df.to_json(postproc_path, orient=\"records\", default_handler=str)\n",
    "    logging.info(f\"Saved to {postproc_path}\")"
   ]
  },
  {
   "source": [
    "### Option 2: Load data directly from df stored on disk"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(postproc_path):\n",
    "    print(\"Loading post-processed data to disk\")\n",
    "    df = pd.read_json(postproc_path, orient=\"records\")"
   ]
  },
  {
   "source": [
    "## Aggregation of region-wide features\n",
    "\n",
    "Niveaux de granularité considérés:\n",
    "- région\n",
    "- département\n",
    "\n",
    "Pour chaque niveau de granularité:\n",
    "- compter le nombre d'établissements flaguées rouge par région\n",
    "- compter le nombre d'établissements flaguées orange par région\n",
    "- compter le nombre d'établissements flaguées en rouge OU en orange\n",
    "- rapporter ces nombre d'établissements au nombre total d'établissements dans la zone géographique\n",
    "- compter le nombre de défaillances effectives sur une période donnée, et calculer le ratio correspondants\n",
    "\n",
    "Pour toutes les grandeurs calculées précédemment, calculer des équivalents en nombre d'employés concernés.\n",
    "\n",
    "Pour les établissements ayant un risque de défaillance modéré ou fort (flagguée en rouge OU en orange), communiquer:\n",
    "- ratio $\\frac{dette_{ouvriere}}{cotisation}$ moyen\n",
    "- recours moyen à l'activité partielle\n",
    "\n",
    "On pourra éventuellement ajouter à ces premières métriques:\n",
    "- des ratios financiers provenant de la DE de la Banque de France\n",
    "- le nombre de jour moyen de retard de paiement aux fournisseurs (donnée Paydex)\n",
    "\n",
    "### Preprocessing steps"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):\n",
    "    # create an outcome flag based only on failures since the beginning of the COVID crisis\n",
    "    df[\"failure\"] = (df[\"time_til_failure\"]>=0) & (df[\"time_til_failure\"]<12) # todo: automatiser le nombre de mois à regarder vers l'avant: entre mars 2020 et <THIS_MONTH>\n",
    "    df[\"failure\"] = df.failure.astype(int)\n",
    "\n",
    "    # encode alert level into integer\n",
    "    df[\"alert_flag\"] = df.alert.replace({\"Pas d'alerte\": 0, \"Alerte seuil F1\": 1, \"Alerte seuil F2\": 2})\n",
    "    df[\"alert_bin\"] = (df.alert_flag > 0)\n",
    "\n",
    "    # ratio dette/cotisation sur la part salariale des cotisations sociales\n",
    "    df[\"ratio_dette_ouvriere\"] = df[\"montant_part_ouvriere\"] / df[\"cotisation\"]\n",
    "    df[\"ratio_dette_patronale\"] = df[\"montant_part_patronale\"] / df[\"cotisation\"]\n",
    "    return df\n",
    "\n",
    "def replace_nans(df, replace_dct):\n",
    "    for field, rpl in replace_dct.items():\n",
    "        df[field].fillna(value=rpl, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_nans(df, cab_config.NAN_RPL)\n",
    "df = preprocess(df)"
   ]
  },
  {
   "source": [
    "Vérification des effectifs par catégorie:\n",
    "- entrée en procédure collective\n",
    "- flagging par l'algorithme SF\n",
    "- flagging binaire par l'algorithme SF (True si un établissement est flagué en rouge OU en orange, False si l'établissement est flagué Vert, ie non flagué)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "failure\n",
       "0    644012\n",
       "1     13284\n",
       "Name: siret, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "df.groupby(by=[\"failure\"]).siret.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "alert_flag\n",
       "0    591740\n",
       "1     37598\n",
       "2     27958\n",
       "Name: siret, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "df.groupby(by=[\"alert_flag\"]).siret.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "alert_bin\n",
       "False    591740\n",
       "True      65556\n",
       "Name: siret, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "df.groupby(by=[\"alert_bin\"]).siret.count()"
   ]
  },
  {
   "source": [
    "### Building aggregation dataframe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cab_config.FEATURES_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_stats(geo_attr, outcome_attr):\n",
    "    assert outcome_attr in [\"alert_flag\", \"alert_bin\", \"failure\", \"outcome\"]\n",
    "\n",
    "    risk_ape3_stats = df.groupby(by=[geo_attr,outcome_attr,\"libelle_naf\",\"libelle_ape3\"]).agg(\n",
    "        siret_count=('siret', 'count'),\n",
    "        effectif_tot=('effectif', 'sum'),\n",
    "    )\n",
    "    risk_naf_stats = df.groupby(by=[geo_attr,outcome_attr,\"libelle_naf\"]).agg(\n",
    "        siret_count=('siret', 'count'),\n",
    "        effectif_tot=('effectif', 'sum')\n",
    "    )\n",
    "    risk_stats = df.groupby(by=[geo_attr,outcome_attr]).agg(\n",
    "        siret_count=('siret', 'count'),\n",
    "        effectif_tot=('effectif', 'sum'),\n",
    "        ratiodette_ouvr_avg=('ratio_dette_ouvriere', 'mean'),\n",
    "        ratiodette_patr_avg=('ratio_dette_patronale', 'mean'),\n",
    "        ratiodette_avg=('ratio_dette', 'mean'),\n",
    "        apart_autr_avg=('apart_heures_autorisees', 'mean'),\n",
    "        apart_cons_avg=('apart_heures_consommees', 'mean'),\n",
    "        apart_cumcons_avg=('apart_heures_consommees_cumulees', 'mean'),\n",
    "        paydex_avg=('paydex_nb_jours', 'mean'),\n",
    "        taux_endettement_avg=('taux_endettement', 'mean'),\n",
    "    )\n",
    "    national_stats = df.groupby(by=[outcome_attr]).agg(\n",
    "        siret_count=('siret', 'count'),\n",
    "        effectif_tot=('effectif', 'sum'),\n",
    "        ratiodette_ouvr_avg=('ratio_dette_ouvriere', 'mean'),\n",
    "        ratiodette_patr_avg=('ratio_dette_patronale', 'mean'),\n",
    "        ratiodette_avg=('ratio_dette', 'mean'),\n",
    "        apart_autr_avg=('apart_heures_autorisees', 'mean'),\n",
    "        apart_cons_avg=('apart_heures_consommees', 'mean'),\n",
    "        apart_cumcons_avg=('apart_heures_consommees_cumulees', 'mean'),\n",
    "        paydex_avg=('paydex_nb_jours', 'mean'),\n",
    "        taux_endettement_avg=('taux_endettement', 'mean'),\n",
    "    )\n",
    "\n",
    "    risk_stats['siret_rate'] = 100*risk_stats.siret_count / risk_stats.groupby(by=geo_attr).siret_count.sum()\n",
    "    risk_stats['effectif_rate'] = 100*risk_stats.effectif_tot / risk_stats.groupby(by=geo_attr).effectif_tot.sum()\n",
    "\n",
    "    risk_stats['ape3_mostatrisk_eff'] = risk_ape3_stats.loc[risk_ape3_stats.index.get_level_values(\"libelle_naf\").isin([LIBELLE_NAF[code] for code in [\"B\", \"C\", \"D\", \"E\"]])].groupby(by=[geo_attr,outcome_attr]).effectif_tot.idxmax()\n",
    "    risk_stats['ape3_mostatrisk_eff'] = risk_stats.ape3_mostatrisk_eff.apply(lambda x: x[3] if isinstance(x, tuple) else \"None\")\n",
    "    risk_stats['ape3_risk_eff'] = risk_ape3_stats.loc[risk_ape3_stats.index.get_level_values(\"libelle_naf\").isin([LIBELLE_NAF[code] for code in [\"B\", \"C\", \"D\", \"E\"]])].groupby(by=[geo_attr,outcome_attr]).effectif_tot.max()\n",
    "\n",
    "    risk_stats['naf_mostatrisk_eff'] = risk_naf_stats.groupby(by=[geo_attr,outcome_attr]).effectif_tot.idxmax()\n",
    "    risk_stats['naf_mostatrisk_eff'] = risk_stats.naf_mostatrisk_eff.apply(lambda x: x[2] if isinstance(x, tuple) else \"None\")\n",
    "    risk_stats['naf_risk_eff'] = risk_naf_stats.groupby(by=[geo_attr,outcome_attr]).effectif_tot.max()\n",
    "\n",
    "    risk_stats['ape3_mostatrisk_etab'] = risk_ape3_stats.loc[risk_ape3_stats.index.get_level_values(\"libelle_naf\").isin([LIBELLE_NAF[code] for code in [\"B\", \"C\", \"D\", \"E\"]])].groupby(by=[geo_attr,outcome_attr]).siret_count.idxmax()\n",
    "    risk_stats['ape3_mostatrisk_etab'] = risk_stats.ape3_mostatrisk_etab.apply(lambda x: x[3] if isinstance(x, tuple) else \"None\")\n",
    "    risk_stats['ape3_risk_etab'] = risk_ape3_stats.loc[risk_ape3_stats.index.get_level_values(\"libelle_naf\").isin([LIBELLE_NAF[code] for code in [\"B\", \"C\", \"D\", \"E\"]])].groupby(by=[geo_attr,outcome_attr]).siret_count.max()\n",
    "\n",
    "    risk_stats['naf_mostatrisk_etab'] = risk_naf_stats.groupby(by=[geo_attr,outcome_attr]).siret_count.idxmax()\n",
    "    risk_stats['naf_mostatrisk_etab'] = risk_stats.naf_mostatrisk_etab.apply(lambda x: x[2] if isinstance(x, tuple) else \"None\")\n",
    "    risk_stats['naf_risk_etab'] = risk_naf_stats.groupby(by=[geo_attr,outcome_attr]).siret_count.max()\n",
    "\n",
    "    national_stats['siret_rate'] = 100*national_stats.siret_count / national_stats.siret_count.sum()\n",
    "    national_stats['effectif_rate'] = 100*national_stats.effectif_tot / national_stats.effectif_tot.sum()\n",
    "\n",
    "\n",
    "    risk_stats['siret_rate_to_ntl_avg'] = 100*(risk_stats.siret_rate - national_stats.siret_rate) / national_stats.siret_rate\n",
    "    risk_stats['effectif_rate_to_ntl_avg'] = 100*(risk_stats.effectif_rate - national_stats.effectif_rate) / national_stats.effectif_rate\n",
    "    risk_stats['ratiodette_ouvr_avg'] = 100*(risk_stats.ratiodette_ouvr_avg - national_stats.ratiodette_ouvr_avg) / national_stats.ratiodette_ouvr_avg\n",
    "    risk_stats['ratiodette_patr_avg'] = 100*(risk_stats.ratiodette_patr_avg - national_stats.ratiodette_patr_avg) / national_stats.ratiodette_patr_avg\n",
    "    risk_stats['ratiodette_avg'] = 100*(risk_stats.ratiodette_avg - national_stats.ratiodette_avg) / national_stats.ratiodette_avg\n",
    "    risk_stats['apart_autr_avg'] = 100*(risk_stats.apart_autr_avg - national_stats.apart_autr_avg) / national_stats.apart_autr_avg\n",
    "    risk_stats['apart_cons_avg_to_ntl_avg'] = 100*(risk_stats.apart_cons_avg - national_stats.apart_cons_avg) / national_stats.apart_cons_avg\n",
    "    risk_stats['apart_cumcons_avg_to_ntl_avg'] = 100*(risk_stats.apart_cumcons_avg - national_stats.apart_cumcons_avg) / national_stats.apart_cumcons_avg\n",
    "    risk_stats['paydex_avg_to_ntl_avg'] = 100*(risk_stats.paydex_avg - national_stats.paydex_avg) / national_stats.paydex_avg\n",
    "    risk_stats['taux_endettement_avg'] = 100*(risk_stats.taux_endettement_avg - national_stats.taux_endettement_avg) / national_stats.taux_endettement_avg\n",
    "\n",
    "    return risk_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_risk_stats = aggregate_stats(geo_attr=\"region\", outcome_attr=\"alert_flag\")\n",
    "reg_riskbin_stats = aggregate_stats(geo_attr=\"region\", outcome_attr=\"alert_bin\")\n",
    "reg_fail_stats = aggregate_stats(geo_attr=\"region\", outcome_attr=\"failure\")\n",
    "\n",
    "dpt_risk_stats = aggregate_stats(geo_attr=\"departement\", outcome_attr=\"alert_flag\")\n",
    "dpt_riskbin_stats = aggregate_stats(geo_attr=\"departement\", outcome_attr=\"alert_bin\")\n",
    "dpt_fail_stats = aggregate_stats(geo_attr=\"departement\", outcome_attr=\"failure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_risk_outpath_root = \"/home/simon.lebastard/predictsignauxfaibles/data/reg_2103\"\n",
    "dpt_risk_outpath_root = \"/home/simon.lebastard/predictsignauxfaibles/data/dpt_2103\"\n",
    "\n",
    "reg_risk_stats.to_csv(f\"{reg_risk_outpath_root}_riskflag.csv\")\n",
    "dpt_risk_stats.to_csv(f\"{dpt_risk_outpath_root}_riskflag.csv\")\n",
    "reg_riskbin_stats.to_csv(f\"{reg_risk_outpath_root}_riskbin.csv\")\n",
    "dpt_riskbin_stats.to_csv(f\"{dpt_risk_outpath_root}_riskbin.csv\")\n",
    "reg_fail_stats.to_csv(f\"{reg_risk_outpath_root}_failures.csv\")\n",
    "dpt_fail_stats.to_csv(f\"{dpt_risk_outpath_root}_failures.csv\")"
   ]
  }
 ]
}