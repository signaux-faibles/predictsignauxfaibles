{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting started with predictsignauxfaibles - Training a logistic regression\n",
    "===\n",
    "\n",
    "In this notebook, we'll focus on using predictsignauxfaibles to train a logistic regression, in a way that much of the code here can be reused to quickly test other models.\\\n",
    "\\\n",
    "In `predictsignauxfaibles`, our models are \"declared and specified\" in `models/<MODEL_NAME>/model_conf.py`\\\n",
    "Our processing pipeline works as following:\n",
    "- fetching input vairables for train, test and prediction (when pertains) sets\n",
    "- pre-processing our data to produce model features\n",
    "- feed this pre-processed data into a model, produce evaluation metrics and predictions\n",
    "- log training/testing/prediction statistics\n",
    "\\\n",
    "Here we will assume that you wish to train a model that uses the same pre-processing steps as in `models/default/model_conf.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import importlib.util\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import fbeta_score, balanced_accuracy_score\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import predictsignauxfaibles.models\n",
    "from predictsignauxfaibles.data import SFDataset\n",
    "from predictsignauxfaibles.config import OUTPUT_FOLDER, IGNORE_NA\n",
    "from predictsignauxfaibles.pipelines import run_pipeline\n",
    "from predictsignauxfaibles.utils import load_conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logging a preprocessing & model configuration\n",
    "---\n",
    "The following code fetches the configuration module for model `default`, so that we can easily access, use and adapt the preprocessing steps, train and test sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = load_conf(\"default\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then look into and modify the current configuration:\n",
    "- `conf.VARIABLES`contains the list of variables to be fetched\n",
    "- `conf.FEATURES` contains the list of features to be produced from those variables during pre-processing steps\n",
    "- `conf.TRANSFO_PIPELINE` contains the pre-processing pipeline, which is a list of `predictsignauxfaibles.Preprocessor` objects. Each preprocessor is defined by a function, a set of inputs and a set of outputs\n",
    "- `conf.MODEL_PIPELINE` contains a `sklearn.pipeline` with `fit` and `predict` methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.VARIABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.TRANSFO_PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.MODEL_PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = conf.TRAIN_DATASET\n",
    "train.sample_size = 1e4\n",
    "\n",
    "test = conf.TEST_DATASET\n",
    "test.sample_size = 1e4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fetching data\n",
    "---\n",
    "At this point, we have allocated datasets but we have not fetched any data into it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.TRAIN_DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Load data from MongoDB (requires an authorized connection to our database):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.fetch_data().raise_if_empty()\n",
    "test.fetch_data().raise_if_empty()\n",
    "logging.info(\"Succesfully loaded Features data from MongoDB\")\n",
    "\n",
    "if True:\n",
    "    savepath = \"path/to/local_features_save.\"\n",
    "    train.data.to_csv(f\"{savepath}_train.csv\")\n",
    "    test.data.to_csv(f\"{savepath}_test.csv\")\n",
    "    logging.info(f\"Saved Features extract to {savepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Load data from a local file, for instance a csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_filepath = \"/path/to/train_dataset.csv\"\n",
    "test_filepath = \"/path/to/test_dataset.csv\"\n",
    "\n",
    "train.data = pd.read_csv(train_filepath)\n",
    "test.data = pd.read_csv(test_filepath)\n",
    "\n",
    "logging.info(\"Succesfully loaded Features data from %s\", features_filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 3: Perform your train/test split a posteriori from a single saved extract from Features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_filepath = \"/home/simon.lebastard/predictsignauxfaibles/data/features_default_10k.csv\" #\"/path/to/features_extract.csv\"\n",
    "df = SFDataset(\n",
    "    date_min=\"2018-01-01\",\n",
    "    date_max=\"2018-12-31\",\n",
    "    fields=conf.VARIABLES,\n",
    "    sample_size=2e4,\n",
    ")\n",
    "df.data = pd.read_csv(features_filepath)\n",
    "\n",
    "X_train, X_test, _, _ = train_test_split(\n",
    "    df.data,\n",
    "    df.data[\"outcome\"],\n",
    "    test_size=0.33,\n",
    "    random_state=42\n",
    ")\n",
    "train = SFDataset()\n",
    "train.data = X_train\n",
    "\n",
    "test = SFDataset()\n",
    "test.data = X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-processing our data\n",
    "---\n",
    "\n",
    "To remove any bias in evaluation, our test set should not contain any SIRET that belong to the same SIREN as any SIRET in train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_siren_set = train.data[\"siren\"].unique().tolist()\n",
    "test.remove_siren(train_siren_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then run the trasnformation (=pre-processing) pipeline on both sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.replace_missing_data().remove_na(ignore=IGNORE_NA)\n",
    "train.data = run_pipeline(train.data, conf.TRANSFO_PIPELINE)\n",
    "\n",
    "test.replace_missing_data().remove_na(ignore=IGNORE_NA)\n",
    "test.data = run_pipeline(test.data, conf.TRANSFO_PIPELINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training our model\n",
    "---\n",
    "To train any model on our data, you can create and modify you own modeling pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pp = conf.MODEL_PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = model_pp.fit(train.data, train.data[\"outcome\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = fit.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model evaluation\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(\n",
    "    model, dataset, beta\n",
    "):  # To be turned into a SFModel method when refactoring models\n",
    "    \"\"\"\n",
    "    Returns evaluation metrics of model evaluated on df\n",
    "    Args:\n",
    "        model: a sklearn-like model with a predict method\n",
    "        df: dataset\n",
    "    \"\"\"\n",
    "    balanced_accuracy = balanced_accuracy_score(\n",
    "        dataset.data[\"outcome\"], model.predict(dataset.data)\n",
    "    )\n",
    "    fbeta = fbeta_score(dataset.data[\"outcome\"], model.predict(dataset.data), beta=beta)\n",
    "    return {\"balanced_accuracy\": balanced_accuracy, \"fbeta\": fbeta}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics = evaluate(fit, test, conf.EVAL_BETA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
