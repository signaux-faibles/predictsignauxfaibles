{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signaux Faibles - Data Science DÃ©mo\n",
    "\n",
    "The purpose of this repo is to get your started using the `predictsignauxfaibles` repository.\n",
    "\n",
    "In this notebook, we will retrieve some data in a `SFDataset` object, train a basic `SFModelGAM` on it and make some predictions using our trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "You should have created a `.env` file at the root of your local copy of the repo. The required entries are documented in `.env.example`. _Never_ commit your `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add root of the repo to PYTHONPATH\n",
    "import sys\n",
    "sys.path.append(\"../.\")\n",
    "\n",
    "# mute warnings (! do not do this when working in prod !)\n",
    "# TODO: fix pyGAM warnings https://github.com/signaux-faibles/predictsignauxfaibles/issues/12\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set logging level to INFO\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Import required libraries and modules\n",
    "import pandas as pd\n",
    "import config\n",
    "from predictsignauxfaibles.data import SFDataset\n",
    "from predictsignauxfaibles.models import SFModelGAM\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that you have access to MongoDB. If you are ensure how to do this, just ask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "The easiest way to load a dataset is via the `SFDataset` class. It can be instantiated in two ways :\n",
    "- via its constructor method `dataset = SFDataset(...)`, better for developping and exploring the data\n",
    "- via a yaml configuration file `dataset = SFDataset.from_config_file(\"../models/rocketscience/model.yml\")`, which is best for ensuring reproducibility and for production use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "Signaux Faibles Dataset (batch_id : 2102)\n",
       "----------------------------------------------------\n",
       "Empty Dataset\n",
       "[...]\n",
       "----------------------------------------------------\n",
       "Number of observations = 0\n",
       "        "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MY_FEATURES = [\n",
    "    \"montant_part_ouvriere_past_1\",\n",
    "    \"montant_part_patronale_past_1\",\n",
    "    \"ratio_dette\",\n",
    "]\n",
    "\n",
    "# It's always a good idea to query periods, siret, and outcomes too\n",
    "FIELDS_TO_QUERY =  [\"siret\", \"siren\", \"periode\", \"outcome\"] + MY_FEATURES\n",
    "\n",
    "dataset = SFDataset(\n",
    "    date_min=\"2015-01-01\",\n",
    "    date_max=\"2016-06-30\",\n",
    "    fields=FIELDS_TO_QUERY,\n",
    "    sample_size=10_000\n",
    ")\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully created an (empty) dataset. Use the `fetch_data` method to fill it. The data is stored as a Pandas DataFrame in the `.data` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>periode</th>\n",
       "      <th>outcome</th>\n",
       "      <th>montant_part_ouvriere_past_1</th>\n",
       "      <th>montant_part_patronale_past_1</th>\n",
       "      <th>ratio_dette</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-04-01</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016-04-01</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-11-01</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016-05-01</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-04-01</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     periode  outcome  montant_part_ouvriere_past_1  \\\n",
       "0 2015-04-01    False                           0.0   \n",
       "1 2016-04-01    False                           0.0   \n",
       "2 2015-11-01    False                           0.0   \n",
       "3 2016-05-01    False                           0.0   \n",
       "4 2015-04-01    False                           0.0   \n",
       "\n",
       "   montant_part_patronale_past_1  ratio_dette  \n",
       "0                            0.0          0.0  \n",
       "1                            0.0          0.0  \n",
       "2                            0.0          NaN  \n",
       "3                            0.0          0.0  \n",
       "4                            0.0          0.0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.fetch_data()\n",
    "\n",
    "# hide siret and siren as the repo is public\n",
    "dataset.data.head().loc[:, ~dataset.data.columns.isin([\"siret\", \"siren\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `prepare_data()` for standard data preprocessing. This method :\n",
    "- creates a `siren` column from the `siret`\n",
    "- fills missing values with their defaults defined in `config.py`\n",
    "- drops any remaining observation with NAs\n",
    "\n",
    "\n",
    "You can also manipulate `dataset.data` yourself if you want to perform your own transformation of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Replacing missing data with default values\n",
      "INFO:root:Drop observations with missing required fields.\n",
      "INFO:root:Removing NAs from dataset.\n",
      "INFO:root:Number of observations before: 10000\n",
      "INFO:root:Number of observations after: 9865\n",
      "INFO:root:Resetting index for DataFrame.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Signaux Faibles Dataset (batch_id : 2102)\n",
       "----------------------------------------------------\n",
       "            siret    periode  outcome  montant_part_ouvriere_past_1  \\\n",
       "0  32657703800028 2015-04-01    False                           0.0   \n",
       "1  32018930100015 2016-04-01    False                           0.0   \n",
       "2  33037752400203 2015-11-01    False                           0.0   \n",
       "3  44937840500012 2016-05-01    False                           0.0   \n",
       "4  77977749900079 2015-04-01    False                           0.0   \n",
       "\n",
       "   montant_part_patronale_past_1      siren  ratio_dette  \n",
       "0                            0.0  326577038          0.0  \n",
       "1                            0.0  320189301          0.0  \n",
       "2                            0.0  330377524          0.0  \n",
       "3                            0.0  449378405          0.0  \n",
       "4                            0.0  779777499          0.0  \n",
       "[...]\n",
       "----------------------------------------------------\n",
       "Number of observations = 9865\n",
       "        "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to load the json file \"variables.json\" to get the entire list of features, if needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 316)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../variables.json\",  encoding='utf-8') as json_file:\n",
    "    doc = json.load(json_file)\n",
    "doc = pd.DataFrame(doc)\n",
    "MY_FEATURES_ALL = list(doc.name.unique())\n",
    "dataset_all = SFDataset(\n",
    "    date_min=\"2015-01-01\",\n",
    "    date_max=\"2016-06-30\",\n",
    "    fields=MY_FEATURES_ALL, # NB: the default value is \"all\" too :)\n",
    "    sample_size=10_000\n",
    ")\n",
    "dataset_all.fetch_data()\n",
    "dataset_all.data.shape\n",
    "# We got all the variables, ie more than 300 features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model\n",
    "\n",
    "Just like datasets, models can be instantiated in two ways :\n",
    "- via its constructor method `dataset = SFModel(...)`, better for developping and exploring the data\n",
    "- via a yaml configuration file `dataset = SFModel.from_config_file(\"../models/rocketscience/model.yml\")`, which is best for ensuring reproducibility and for production use.\n",
    "\n",
    "Once you are done developping a new model, don't forget to write your configuration file so that your coworkers can reproduce and audit your work :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "gam = SFModelGAM(dataset, features=MY_FEATURES, target=\"outcome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a model using its `train` method. The (trained) model is stored in the `.model` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticGAM                                                                                               \n",
      "=============================================== ==========================================================\n",
      "Distribution:                      BinomialDist Effective DoF:                                      6.1966\n",
      "Link Function:                        LogitLink Log Likelihood:                                  -979.1556\n",
      "Number of Samples:                         9865 AIC:                                             1970.7044\n",
      "                                                AICc:                                            1970.7164\n",
      "                                                UBRE:                                               2.2003\n",
      "                                                Scale:                                                 1.0\n",
      "                                                Pseudo R-Squared:                                   0.4898\n",
      "==========================================================================================================\n",
      "Feature Function                  Lambda               Rank         EDoF         P > x        Sig. Code   \n",
      "================================= ==================== ============ ============ ============ ============\n",
      "s(0)                              [0.6]                20           3.9          2.01e-06     ***         \n",
      "s(1)                              [0.6]                20           1.5          0.00e+00     ***         \n",
      "s(2)                              [0.6]                20           0.8          0.00e+00     ***         \n",
      "intercept                                              1            0.0          0.00e+00     ***         \n",
      "==========================================================================================================\n",
      "Significance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
      "\n",
      "WARNING: Fitting splines and a linear function to a feature introduces a model identifiability problem\n",
      "         which can cause p-values to appear significant when they are not.\n",
      "\n",
      "WARNING: p-values calculated in this manner behave correctly for un-penalized models or models with\n",
      "         known smoothing parameters, but when smoothing parameters have been estimated, the p-values\n",
      "         are typically lower than they should be, meaning that the tests reject the null too readily.\n"
     ]
    }
   ],
   "source": [
    "gam.train()\n",
    "gam.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model\n",
    "\n",
    "Signaux Faible uses a fairly specific way to evaluate a model. This evaluation process is implemented in `SFModelEvaluator`.\n",
    "\n",
    "First, start by querying a validation dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Replacing missing data with default values\n",
      "INFO:root:Drop observations with missing required fields.\n",
      "INFO:root:Removing NAs from dataset.\n",
      "INFO:root:Number of observations before: 5000\n",
      "INFO:root:Number of observations after: 4939\n",
      "INFO:root:Resetting index for DataFrame.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Signaux Faibles Dataset (batch_id : 2102)\n",
       "----------------------------------------------------\n",
       "            siret    periode  outcome  montant_part_ouvriere_past_1  \\\n",
       "0  78439368800063 2018-02-01    False                           0.0   \n",
       "1  32992501093876 2018-03-01    False                           0.0   \n",
       "2  57980541700022 2018-03-01    False                           0.0   \n",
       "3  52289945900021 2018-02-01    False                           0.0   \n",
       "4  38994147700041 2018-05-01    False                           0.0   \n",
       "\n",
       "   montant_part_patronale_past_1      siren  ratio_dette  \n",
       "0                            0.0  784393688          0.0  \n",
       "1                            0.0  329925010          0.0  \n",
       "2                            0.0  579805417          0.0  \n",
       "3                            0.0  522899459          0.0  \n",
       "4                            0.0  389941477          0.0  \n",
       "[...]\n",
       "----------------------------------------------------\n",
       "Number of observations = 4939\n",
       "        "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_set = SFDataset(\n",
    "        date_min=\"2018-01-01\",\n",
    "        date_max=\"2018-06-30\",\n",
    "        fields=FIELDS_TO_QUERY,\n",
    "        sample_size=5_000\n",
    ")\n",
    "validation_set.fetch_data().prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cross-validation evaluation method on our model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.5805391396545401,\n",
       " 1: 0.5912488741294926,\n",
       " 2: 0.5889695609995358,\n",
       " 3: 0.6007928337183918,\n",
       " 4: 0.5979756981215532}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from predictsignauxfaibles.model_selection import SFModelEvaluator\n",
    "\n",
    "cv_scores = SFModelEvaluator(model = gam).cv_evaluation(num_folds=5, validate_set=validation_set)\n",
    "cv_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the average performance of the model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.592"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_score = sum(cv_scores.values()) / len(cv_scores)\n",
    "round(average_score, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.01483678, 0.01483678, 0.01483678, 0.01483678, 0.01483678]),\n",
       " array([False, False, False, False, False]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = validation_set.data[MY_FEATURES]\n",
    "\n",
    "# predict probabilities (a float)\n",
    "pred_probas = gam.predict_proba(new_data)\n",
    "\n",
    "# predict outcome (True/False)\n",
    "pred_outcomes = gam.predict(new_data)\n",
    "\n",
    "pred_probas[:5], pred_outcomes[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model\n",
    "\n",
    "Work in progress :) In the meantime, you can use `pickle` to serialize any python object."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}