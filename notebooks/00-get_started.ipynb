{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signaux Faibles - Data Science DÃ©mo\n",
    "\n",
    "The purpose of this repo is to get your started using the `predictsignauxfaibles` repository.\n",
    "\n",
    "In this notebook, we will retrieve some data in a `SFDataset` object, train a basic `SFModelGAM` on it and make some predictions using our trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "You should have created a `.env` file at the root of your local copy of the repo. The required entries are documented in `.env.example`. _Never_ commit your `.env` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add root of the repo to PYTHONPATH\n",
    "import sys\n",
    "sys.path.append(\"../.\")\n",
    "\n",
    "# mute warnings (! do not do this when working in prod !)\n",
    "# TODO: fix pyGAM warnings https://github.com/signaux-faibles/predictsignauxfaibles/issues/12\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set logging level to INFO\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Import required libraries and modules\n",
    "import pandas as pd\n",
    "import predictsignauxfaibles.config as config\n",
    "from predictsignauxfaibles.data import SFDataset\n",
    "from predictsignauxfaibles.models import SFModelGAM\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that you have access to MongoDB. If you are ensure how to do this, just ask."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "The easiest way to load a dataset is via the `SFDataset` class. It can be instantiated in two ways :\n",
    "- via its constructor method `dataset = SFDataset(...)`, better for developping and exploring the data\n",
    "- via a yaml configuration file `dataset = SFDataset.from_config_file(\"../models/rocketscience/model.yml\")`, which is best for ensuring reproducibility and for production use.\n",
    "\n",
    "There is also a `OversampledSFDataset` class available that lets your ask for a given proportion of positiuve observations in the resulting dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from predictsignauxfaibles.data import SFDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_FEATURES = [\n",
    "    \"montant_part_ouvriere_past_1\",\n",
    "    \"montant_part_patronale_past_1\",\n",
    "    \"ratio_dette\",\n",
    "]\n",
    "\n",
    "# It's always a good idea to query periods, siret, and outcomes too\n",
    "FIELDS_TO_QUERY =  [\"siret\", \"siren\", \"periode\", \"outcome\"] + MY_FEATURES\n",
    "\n",
    "dataset = SFDataset(\n",
    "    date_min=\"2015-01-01\",\n",
    "    date_max=\"2020-06-30\",\n",
    "    fields=FIELDS_TO_QUERY,\n",
    "    sample_size=100\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully created an (empty) dataset. Use the `fetch_data` method to fill it. The data is stored as a Pandas DataFrame in the `.data` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.fetch_data()\n",
    "\n",
    "# show first 5 rows of dataset\n",
    "dataset.data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run `prepare_data()` for standard data preprocessing. This method :\n",
    "- fills missing values with their defaults defined in `config.py`\n",
    "- drops any remaining observation with NAs\n",
    "- optionally removes \"strong signals\"\n",
    "\n",
    "\n",
    "You can also manipulate `dataset.data` yourself if you want to perform your own transformation of the data. Look into the `predictsignauxfaibles.preprocessors` for common preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to load the json file \"variables.json\" to get the entire list of features, if needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../variables.json\",  encoding='utf-8') as json_file:\n",
    "    doc = json.load(json_file)\n",
    "doc = pd.DataFrame(doc)\n",
    "MY_FEATURES_ALL = list(doc.name.unique())\n",
    "dataset_all = SFDataset(\n",
    "    date_min=\"2015-01-01\",\n",
    "    date_max=\"2016-06-30\",\n",
    "    fields=MY_FEATURES_ALL, # NB: the default value is \"all\" too :)\n",
    "    sample_size=10_000\n",
    ")\n",
    "dataset_all.fetch_data()\n",
    "dataset_all.data.shape\n",
    "# We got all the variables, ie more than 300 features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a model\n",
    "\n",
    "Just like datasets, models can be instantiated in two ways :\n",
    "- via its constructor method `dataset = SFModel(...)`, better for developping and exploring the data\n",
    "- via a yaml configuration file `dataset = SFModel.from_config_file(\"../models/rocketscience/model.yml\")`, which is best for ensuring reproducibility and for production use.\n",
    "\n",
    "Once you are done developping a new model, don't forget to write your configuration file so that your coworkers can reproduce and audit your work :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gam = SFModelGAM(dataset, features=MY_FEATURES, target=\"outcome\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a model using its `train` method. The (trained) model is stored in the `.model` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gam.train()\n",
    "gam.model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model\n",
    "\n",
    "Signaux Faible uses a fairly specific way to evaluate a model. This evaluation process is implemented in `SFModelEvaluator`.\n",
    "\n",
    "First, start by querying a validation dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_set = SFDataset(\n",
    "        date_min=\"2018-01-01\",\n",
    "        date_max=\"2018-06-30\",\n",
    "        fields=FIELDS_TO_QUERY,\n",
    "        sample_size=5_000\n",
    ")\n",
    "validation_set.fetch_data().prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cross-validation evaluation method on our model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from predictsignauxfaibles.model_selection import SFModelEvaluator\n",
    "\n",
    "cv_scores = SFModelEvaluator(model = gam).cv_evaluation(num_folds=5, validate_set=validation_set)\n",
    "cv_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the average performance of the model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_score = sum(cv_scores.values()) / len(cv_scores)\n",
    "round(average_score, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictions on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = validation_set.data[MY_FEATURES]\n",
    "\n",
    "# predict probabilities (a float)\n",
    "pred_probas = gam.predict_proba(new_data)\n",
    "\n",
    "# predict outcome (True/False)\n",
    "pred_outcomes = gam.predict(new_data)\n",
    "\n",
    "pred_probas[:5], pred_outcomes[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model\n",
    "\n",
    "Work in progress :) In the meantime, you can use `pickle` to serialize any python object."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
