{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "structural-logistics",
   "metadata": {},
   "source": [
    "# Model evaluation - Optimal thresholding on default probas\n",
    "In this notebook, we use a LogReg trained on 1M SIRETs, evaluated on 250k SIRETs.<br>\n",
    "We compute $f_{\\beta}$ scores, balanced accuracy and select thresholds optimally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-continuity",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "social-thriller",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging level to INFO\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "# Import required libraries and modules\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import json\n",
    "from types import ModuleType\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_curve, fbeta_score, average_precision_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "from predictsignauxfaibles.config import IGNORE_NA\n",
    "from predictsignauxfaibles.data import SFDataset\n",
    "from predictsignauxfaibles.evaluate import evaluate, make_precision_recall_curve, make_thresholds_from_fbeta, make_thresholds_from_conditions\n",
    "from predictsignauxfaibles.make_list import merge_models, assign_flag, make_alert\n",
    "from predictsignauxfaibles.pipelines import run_pipeline\n",
    "from predictsignauxfaibles.utils import load_conf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-migration",
   "metadata": {},
   "source": [
    "## Loading predictions from csv, splitting models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-cancellation",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOADING MODELS FROM FILES\n",
    "\n",
    "model_runs = {\n",
    "    \"default\": \"/home/simon.lebastard/predictsignauxfaibles/predictsignauxfaibles/model_runs/default_20210519-172348\",\n",
    "    \"small\": \"/home/simon.lebastard/predictsignauxfaibles/predictsignauxfaibles/model_runs/small_20210519-172421\"\n",
    "}\n",
    "\n",
    "## DEFAULT MODEL PIPELINE\n",
    "\n",
    "default = pd.read_csv(f\"{model_runs['default']}/predictions.csv\")\n",
    "default_mapper_unpickled =  pickle.load(\n",
    "    open( f\"{model_runs['default']}/model_comp0.pickle\",\n",
    "        \"rb\"\n",
    "        )\n",
    ")\n",
    "default_mapper = default_mapper_unpickled[1]\n",
    "default_model_unpickled = pickle.load(\n",
    "    open( f\"{model_runs['default']}/model_comp1.pickle\",\n",
    "        \"rb\"\n",
    "        )\n",
    ")\n",
    "default_model = default_model_unpickled[1]\n",
    "\n",
    "default_pp = Pipeline(\n",
    "    [(\"transform_dataframe\", default_mapper), (\"fit_model\", default_model)]\n",
    ")\n",
    "\n",
    "## SMALL MODEL PIPELINE\n",
    "\n",
    "small = pd.read_csv(f\"{model_runs['small']}/predictions.csv\")\n",
    "small_mapper_unpickled = pickle.load(\n",
    "    open( f\"{model_runs['small']}/model_comp0.pickle\",\n",
    "        \"rb\"\n",
    "        )\n",
    ")\n",
    "small_mapper = small_mapper_unpickled[1]\n",
    "small_model_unpickled = pickle.load(\n",
    "    open( f\"{model_runs['small']}/model_comp1.pickle\",\n",
    "        \"rb\"\n",
    "        )\n",
    ")\n",
    "small_model = small_model_unpickled[1]\n",
    "\n",
    "small_pp = Pipeline(\n",
    "    [(\"transform_dataframe\", small_mapper), (\"fit_model\", small_model)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-artwork",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_conf = load_conf(model_name=\"default\")\n",
    "test = default_conf.TEST_DATASET\n",
    "test.sample_size = 2.5e5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nominated-creek",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test.fetch_data()\n",
    "test.data = pd.read_csv(\"/home/simon.lebastard/predictsignauxfaibles/data/prod_mars2021/2103_prod_data_test.csv\") #\"/home/simon.lebastard/predictsignauxfaibles/data/prod_mars2021/2103_prod_data_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-avenue",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.replace_missing_data().remove_na(ignore=IGNORE_NA)\n",
    "test.data = run_pipeline(test.data, default_conf.TRANSFO_PIPELINE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-medicine",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_unmapped = test.data[set(test.data.columns).difference(set([\"outcome\"]))]\n",
    "test_outcomes = test.data[\"outcome\"].astype(int).to_numpy()\n",
    "test_features = default_mapper.transform(test_features_unmapped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-democrat",
   "metadata": {},
   "source": [
    "Isolating transformed features on one hand, outcomes on the other:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contained-exclusion",
   "metadata": {},
   "source": [
    "A couple of quicks checks/diagnoses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-malpractice",
   "metadata": {},
   "outputs": [],
   "source": [
    "default.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-extra",
   "metadata": {},
   "outputs": [],
   "source": [
    "small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-deficit",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-capacity",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.data.sample(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-rhythm",
   "metadata": {},
   "outputs": [],
   "source": [
    "100*test.data.groupby(by=\"outcome\").siret.count()/len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "maritime-insider",
   "metadata": {},
   "source": [
    "## Computing precision/recall graph as a function of classification thresholds - Default model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-drawing",
   "metadata": {},
   "source": [
    "Let's load a test set from which we can use the output to measure our performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-twenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresh = make_precision_recall_curve(\n",
    "    test,\n",
    "    default_pp\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-overall",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-trinity",
   "metadata": {},
   "source": [
    "### Computing AUCPR\n",
    "Computing the Area Under Curve fopr Precision-Recall curve (AUCPR), a metric which summarizes the potential of the model itself, without specifying the hyperparameter tuning that must be led to weight the relative importance of  precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-worst",
   "metadata": {},
   "outputs": [],
   "source": [
    "aucpr = average_precision_score(test_outcomes, default_model.predict_proba(test_features)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-corner",
   "metadata": {},
   "outputs": [],
   "source": [
    "aucpr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "visible-techno",
   "metadata": {},
   "source": [
    "## Option 1: Determine the thresholds by hand, looking at the Type2-Type1 errors plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-parcel",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "type_2 = 1 - recall\n",
    "type_1 = 1 - precision\n",
    "\n",
    "ax.scatter(\n",
    "    type_1,\n",
    "    type_2,\n",
    "    label = \"logreg\"\n",
    ");\n",
    "\n",
    "plt.axhline(y=0.37, color='orange', linestyle='-')\n",
    "plt.axvline(x=0.07, color='red', linestyle='-')\n",
    "\n",
    "ax.set_xlabel('Type 1 error', fontsize=16)\n",
    "ax.set_ylabel('Type 2 error', fontsize=16)\n",
    "\n",
    "ax.legend(fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proper-export",
   "metadata": {},
   "source": [
    "On peut sélectionner un seuil F1 (rouge) et un seuil F2 (orange) à partir de cette courbe:\n",
    "- pour la liste F1, on cherche à maximiser la précision, puisqe la liste en question doit être la plus pertinent possible et contenir des entreprises \"prioritaires\". Cette liste minimise donc les faux positifs, favorise la précision et minimise donc l'erreur de type 1.\n",
    "- la liste F2 doit se charger de minimiser les faux négatifs, et est donc plus orienter sur la maximisation du recall. Il s'agit ici de minimiser l'erreur de type 2.\n",
    "\n",
    "Pour $t_{F1}$, on choisit une valeur qui donne une erreur de type 1 petite tout en étant la plus indulgente possible pour l'erreur de type 2. C'est typiquement ce qu'un point de courbure maximale va réaliser. Ici $t_{F1}=0.07$ semble assez satisfaisant (ligne verticale rouge ci-dessus).\n",
    "\n",
    "Pour $t_{F2}$, on choisir au contraire une valeur de courbe (négative) minimale, tout en satisfaisant déjà une erreur de type 2 suffisament faible. Ici $t_{F2}=0.37$ semble assez satisfaisant (ligne horizontale orange ci-dessus)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-academy",
   "metadata": {},
   "source": [
    "## Option 2: Select thresholds by maximising a f-beta (with beta specified for each alert level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complicated-scanning",
   "metadata": {},
   "outputs": [],
   "source": [
    "(t_F1, t_F2) = make_thresholds_from_fbeta(\n",
    "    test_features,\n",
    "    test_outcomes,\n",
    "    default_model,\n",
    "    beta_F1 = 0.5,\n",
    "    beta_F2 = 2,\n",
    "    n_thr = 500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "following-bathroom",
   "metadata": {},
   "source": [
    "Sur le dataset de test chargé depuis /home/simon.lebastard/predictsignauxfaibles/data/prod_mars2021/2103_prod_data_test.csv (250k):<br>\n",
    "F1 - $\\beta=0.5$ - Optimal threshold: $t_{F1}=0.876$ - $f_{0.5}=0.683$<br>\n",
    "F2 - $\\beta=2$ - Optimal threshold: $t_{F2}=0.220$ - $f_{2}=0.495$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "behind-period",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(\n",
    "    default_pp,\n",
    "    test,\n",
    "    beta=2,\n",
    "    thresh=0.22\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mobile-frederick",
   "metadata": {},
   "source": [
    "## 3. Select threshold by specifying a minimal precision for T1 alerts and a minimal recall for T1 + T2 alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-jamaica",
   "metadata": {},
   "outputs": [],
   "source": [
    "(t_F1_c, t_F2_c) = make_thresholds_from_conditions(\n",
    "    precision,\n",
    "    recall,\n",
    "    thresh,\n",
    "    min_precision_F1 = 0.93,\n",
    "    min_recall_F2 = 0.63\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-arthur",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_F1_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-fourth",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_F2_c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-process",
   "metadata": {},
   "source": [
    "Sur le dataset de test chargé depuis /home/simon.lebastard/predictsignauxfaibles/data/prod_mars2021/2103_prod_data_test.csv (250k):<br>\n",
    "F1 - $t_{F1}=0.921$ garantit une précision de 93% pour la liste F1 (rouge)<br>\n",
    "F2 - $t_{F2}=0.124$ garantit un recall de 63% pour la liste F2 (orange)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expired-pizza",
   "metadata": {},
   "source": [
    "## To go further - Can we build a threshold selection criterion based on the variations of recall % threhold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-shareware",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compute the first-order derivative of the type-2 error\n",
    "fod_recall = (recall[:-1] - np.roll(recall[:-1],1))/(thresh - np.roll(thresh,1))\n",
    "fod_recall = fod_recall[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-thousand",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(fod_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electrical-catholic",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chinese-rebate",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "ax.scatter(\n",
    "    thresh[:-81],\n",
    "    fod_recall[:-80],\n",
    "    label = \"logreg\"\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "primary-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    "recalL_maxvar_id = np.argmin(fod_recall[:-80])\n",
    "recall_maxvar = fod_recall[recalL_maxvar_id]\n",
    "\n",
    "recall_maxvar_precision = precision[recalL_maxvar_id]\n",
    "recall_maxvar_recall = recall[recalL_maxvar_id]\n",
    "recall_maxvar_thresh = thresh[recalL_maxvar_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outdoor-blackberry",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_maxvar_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incident-growing",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_maxvar_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unexpected-opera",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_maxvar_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-cancer",
   "metadata": {},
   "source": [
    "For now, we'll stick to manual selection based on the two types of error above, or maybe based on the maximisation of Fbeta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
