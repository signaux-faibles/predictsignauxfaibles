{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "alive-interface",
   "metadata": {},
   "source": [
    "# Model evaluation - Optimal thresholding on default probas\n",
    "In this notebook, we use a LogReg trained on 1M SIRETs, evaluated on 250k SIRETs.<br>\n",
    "We compute $f_{\\beta}$ scores, balanced accuracy and select thresholds optimally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-ethernet",
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set logging level to INFO\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "# Import required libraries and modules\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import json\n",
    "from types import ModuleType\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import precision_recall_curve, fbeta_score, average_precision_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "from predictsignauxfaibles.config import IGNORE_NA\n",
    "from predictsignauxfaibles.data import SFDataset\n",
    "from predictsignauxfaibles.evaluate import evaluate, make_precision_recall_curve, make_thresholds_from_fbeta, make_thresholds_from_conditions\n",
    "from predictsignauxfaibles.merge_models import merge_models\n",
    "from predictsignauxfaibles.pipelines import run_pipeline\n",
    "from predictsignauxfaibles.utils import assign_flag, load_conf, log_splits_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-sauce",
   "metadata": {},
   "source": [
    "### Loading predictions from csv, splitting models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-provision",
   "metadata": {},
   "outputs": [],
   "source": [
    "## LOADING MODELS FROM FILES\n",
    "\n",
    "model_runs = {\n",
    "    \"default\": \"/home/simon.lebastard/predictsignauxfaibles/predictsignauxfaibles/model_runs/default_20210531-165745\",\n",
    "    \"small\": \"/home/simon.lebastard/predictsignauxfaibles/predictsignauxfaibles/model_runs/small_20210531-180946\"\n",
    "}\n",
    "\n",
    "## DEFAULT MODEL PIPELINE\n",
    "\n",
    "default = pd.read_csv(f\"{model_runs['default']}/predictions.csv\")\n",
    "default_mapper_unpickled =  pickle.load(\n",
    "    open( f\"{model_runs['default']}/model_comp0.pickle\",\n",
    "        \"rb\"\n",
    "        )\n",
    ")\n",
    "default_mapper = default_mapper_unpickled[1]\n",
    "default_model_unpickled = pickle.load(\n",
    "    open( f\"{model_runs['default']}/model_comp1.pickle\",\n",
    "        \"rb\"\n",
    "        )\n",
    ")\n",
    "default_model = default_model_unpickled[1]\n",
    "\n",
    "default_pp = Pipeline(\n",
    "    [(\"transform_dataframe\", default_mapper), (\"fit_model\", default_model)]\n",
    ")\n",
    "\n",
    "## SMALL MODEL PIPELINE\n",
    "\n",
    "small = pd.read_csv(f\"{model_runs['small']}/predictions.csv\")\n",
    "small_mapper_unpickled = pickle.load(\n",
    "    open( f\"{model_runs['small']}/model_comp0.pickle\",\n",
    "        \"rb\"\n",
    "        )\n",
    ")\n",
    "small_mapper = small_mapper_unpickled[1]\n",
    "small_model_unpickled = pickle.load(\n",
    "    open( f\"{model_runs['small']}/model_comp1.pickle\",\n",
    "        \"rb\"\n",
    "        )\n",
    ")\n",
    "small_model = small_model_unpickled[1]\n",
    "\n",
    "small_pp = Pipeline(\n",
    "    [(\"transform_dataframe\", small_mapper), (\"fit_model\", small_model)]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-equity",
   "metadata": {},
   "source": [
    "A couple of quicks checks/diagnoses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-working",
   "metadata": {},
   "outputs": [],
   "source": [
    "default.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "traditional-ordinance",
   "metadata": {},
   "outputs": [],
   "source": [
    "small.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-rating",
   "metadata": {},
   "source": [
    "## Choosing model to select thresholds on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expected-chuck",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"default\"\n",
    "\n",
    "if model_name == \"default\":\n",
    "    conf = load_conf(model_name=\"default\")\n",
    "    pp = default_pp\n",
    "    mapper = default_mapper\n",
    "    model = default_model\n",
    "if model_name == \"small\":\n",
    "    conf = load_conf(model_name=\"small\")\n",
    "    pp = small_pp\n",
    "    mapper = small_mapper\n",
    "    model = small_model   \n",
    "\n",
    "test = conf.TEST_DATASET\n",
    "test.sample_size = 1.2e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-friend",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test.fetch_data()\n",
    "test.data = pd.read_csv(\"/home/common/benchmark/052021_full_data_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documented-segment",
   "metadata": {},
   "source": [
    "## Threshold selection for default model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "applied-tuesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.replace_missing_data().remove_na(ignore=IGNORE_NA)\n",
    "test.data = run_pipeline(test.data, conf.TRANSFO_PIPELINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-apartment",
   "metadata": {},
   "source": [
    "Isolating transformed features on one hand, outcomes on the other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thorough-administration",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_unmapped = test.data[set(test.data.columns).difference(set([\"outcome\"]))]\n",
    "test_outcomes = test.data[\"outcome\"].astype(int).to_numpy()\n",
    "test_features = mapper.transform(test_features_unmapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-abraham",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-matthew",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.data.sample(n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expired-journalist",
   "metadata": {},
   "outputs": [],
   "source": [
    "100*test.data.groupby(by=\"outcome\").siret.count()/len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-drove",
   "metadata": {},
   "source": [
    "### Computing precision/recall graph as a function of classification thresholds - Default model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-parameter",
   "metadata": {},
   "source": [
    "Let's load a test set from which we can use the output to measure our performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "configured-adaptation",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresh = make_precision_recall_curve(\n",
    "    test,\n",
    "    pp\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bright-worst",
   "metadata": {},
   "source": [
    "### Computing AUCPR\n",
    "Computing the Area Under Curve fopr Precision-Recall curve (AUCPR), a metric which summarizes the potential of the model itself, without specifying the hyperparameter tuning that must be led to weight the relative importance of  precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-cologne",
   "metadata": {},
   "outputs": [],
   "source": [
    "aucpr = average_precision_score(test_outcomes, model.predict_proba(test_features)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ignored-volume",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Area under Precision-Recall curve: {0:.3f}\".format(aucpr)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlling-azerbaijan",
   "metadata": {},
   "source": [
    "### Option 1: Determine the thresholds by hand, looking at the Type2-Type1 errors plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qualified-enemy",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "type_2 = 1 - recall\n",
    "type_1 = 1 - precision\n",
    "\n",
    "ax.scatter(\n",
    "    type_1,\n",
    "    type_2,\n",
    "    label = \"logreg\"\n",
    ");\n",
    "\n",
    "selected_F1_T1error = 0.09\n",
    "selected_F1_precision = 1 - selected_F1_T1error\n",
    "\n",
    "selected_F2_T2error = 0.415\n",
    "selected_F2_recall = 1 - selected_F2_T2error\n",
    "\n",
    "plt.axhline(y=selected_F2_T2error, color='orange', linestyle='-')\n",
    "plt.axvline(x=selected_F1_T1error, color='red', linestyle='-')\n",
    "\n",
    "ax.set_xlabel('Type 1 error', fontsize=16)\n",
    "ax.set_ylabel('Type 2 error', fontsize=16)\n",
    "\n",
    "ax.legend(fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caring-driving",
   "metadata": {},
   "source": [
    "On peut sélectionner un seuil F1 (rouge) et un seuil F2 (orange) à partir de cette courbe:\n",
    "- pour la liste F1, on cherche à maximiser la précision, puisqe la liste en question doit être la plus pertinent possible et contenir des entreprises \"prioritaires\". Cette liste minimise donc les faux positifs, favorise la précision et minimise donc l'erreur de type 1.\n",
    "- la liste F2 doit se charger de minimiser les faux négatifs, et est donc plus orienter sur la maximisation du recall. Il s'agit ici de minimiser l'erreur de type 2.\n",
    "\n",
    "Pour $t_{F1}$, on choisit une valeur qui donne une erreur de type 1 petite tout en étant la plus indulgente possible pour l'erreur de type 2. C'est typiquement ce qu'un point de courbure maximale va réaliser. Ici $t_{F1}=0.07$ semble assez satisfaisant (ligne verticale rouge ci-dessus).\n",
    "\n",
    "Pour $t_{F2}$, on choisir au contraire une valeur de courbe (négative) minimale, tout en satisfaisant déjà une erreur de type 2 suffisament faible. Ici $t_{F2}=0.37$ semble assez satisfaisant (ligne horizontale orange ci-dessus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-involvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "(t_F1_cond, t_F2_cond) = make_thresholds_from_conditions(\n",
    "    precision,\n",
    "    recall,\n",
    "    thresh,\n",
    "    min_precision_F1 = selected_F1_precision,\n",
    "    min_recall_F2 = selected_F2_recall,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-garage",
   "metadata": {},
   "source": [
    "#### DEFAULT model\n",
    "Sur le dataset de test chargé depuis `/home/common/benchmark/052021_full_data_test.csv` (1.2M):<br>\n",
    "F1 - $t_{F1}=0.904$ garantit une précision de 91% pour la liste F1 (rouge)<br>\n",
    "F2 - $t_{F2}=0.134$ garantit un recall de 58.5% pour la liste F2 (orange)\n",
    "\n",
    "#### SMALL model\n",
    "Pour le modèle \"small“, sur le dataset de test chargé depuis `/home/common/benchmark/052021_full_data_test.csv`, la courbe précision/recall est trop \"bruitée\" pour une sélection de seuil semi-manuelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-induction",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_splits_size(\n",
    "    model.predict_proba(test_features)[:, 1],\n",
    "    t_rouge=t_F1_cond,\n",
    "    t_orange=t_F2_cond,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "later-summer",
   "metadata": {},
   "source": [
    "#### DEFAULT model\n",
    "Avec les seuils sélectionnés juste au-dessus par une sélection semi-manuelle:\n",
    "- 7662 risque fort (1.53%)\n",
    "- 35913 risque modéré (7.15%)\n",
    "\n",
    "#### SMALL model\n",
    "Pour le modèle \"small“, sur le dataset de test chargé depuis `/home/common/benchmark/052021_full_data_test.csv`, la courbe précision/recall est trop \"bruitée\" pour une sélection de seuil semi-manuelle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-brand",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(\n",
    "    pp,\n",
    "    test,\n",
    "    beta=0.5,\n",
    "    thresh=t_F1_cond\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stopped-minneapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(\n",
    "    pp,\n",
    "    test,\n",
    "    beta=2,\n",
    "    thresh=t_F2_cond\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "british-rhythm",
   "metadata": {},
   "source": [
    "### Option 2: Select thresholds by maximising a f-beta (with beta specified for each alert level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "burning-library",
   "metadata": {},
   "outputs": [],
   "source": [
    "(t_F1_fb, t_F2_fb) = make_thresholds_from_fbeta(\n",
    "    test_features,\n",
    "    test_outcomes,\n",
    "    model,\n",
    "    beta_F1 = 0.5,\n",
    "    beta_F2 = 2,\n",
    "    n_thr = 1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "textile-dispatch",
   "metadata": {},
   "source": [
    "#### DEFAULT model\n",
    "Sur le dataset de test chargé depuis `/home/common/benchmark/052021_full_data_test.csv` (1.2M):<br>\n",
    "F1 - $\\beta=0.5$ - Optimal threshold: $t_{F1}=0.868$ - $f_{0.5}=0.672$<br>\n",
    "F2 - $\\beta=2$ - Optimal threshold: $t_{F2}=0.183$ - $f_{2}=0.495$\n",
    "\n",
    "#### SMALL model\n",
    "Sur le dataset de test chargé depuis `/home/common/benchmark/052021_full_data_test.csv` (1.2M):<br>\n",
    "F1 - $\\beta=0.5$ - Optimal threshold: $t_{F1}=0.684$ - $f_{0.5}=0.670$<br>\n",
    "F2 - $\\beta=2$ - Optimal threshold: $t_{F2}=0.127$ - $f_{2}=0.514$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-korea",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_splits_size(\n",
    "    model.predict_proba(test_features)[:, 1],\n",
    "    t_rouge=t_F1_fb,\n",
    "    t_orange=t_F2_fb,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "micro-concrete",
   "metadata": {},
   "source": [
    "#### DEFAULT model\n",
    "Avec les seuils sélectionnés juste au-dessus par une maximisation des f_0.5 et f_2 scores, respectivement:\n",
    "- 8131 risque fort (1.62%)\n",
    "- 15056 risque modéré (3.0%)\n",
    "\n",
    "#### SMALL model\n",
    "Avec les seuils sélectionnés juste au-dessus par une maximisation des f_0.5 et f_2 scores, respectivement:\n",
    "- 8425 risque fort (1.68%)\n",
    "- 13640 risque modéré (2.72%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confused-shoulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(\n",
    "    pp,\n",
    "    test,\n",
    "    beta=0.5,\n",
    "    thresh=t_F1_fb\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-baghdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(\n",
    "    pp,\n",
    "    test,\n",
    "    beta=2,\n",
    "    thresh=t_F2_fb\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "australian-halifax",
   "metadata": {},
   "source": [
    "## To go further - Can we build a threshold selection criterion based on the variations of recall % threhold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "warming-forest",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's compute the first-order derivative of the type-2 error\n",
    "fod_recall = (recall[:-1] - np.roll(recall[:-1],1))/(thresh - np.roll(thresh,1))\n",
    "fod_recall = fod_recall[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "designed-beaver",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(fod_recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-litigation",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-drill",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10,10))\n",
    "\n",
    "ax.scatter(\n",
    "    thresh[:-81],\n",
    "    fod_recall[:-80],\n",
    "    label = \"logreg\"\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "characteristic-beatles",
   "metadata": {},
   "outputs": [],
   "source": [
    "recalL_maxvar_id = np.argmin(fod_recall[:-80])\n",
    "recall_maxvar = fod_recall[recalL_maxvar_id]\n",
    "\n",
    "recall_maxvar_precision = precision[recalL_maxvar_id]\n",
    "recall_maxvar_recall = recall[recalL_maxvar_id]\n",
    "recall_maxvar_thresh = thresh[recalL_maxvar_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "analyzed-anger",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_maxvar_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-logging",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_maxvar_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-parks",
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_maxvar_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "talented-determination",
   "metadata": {},
   "source": [
    "For now, we'll stick to manual selection based on the two types of error above, or maybe based on the maximisation of Fbeta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
