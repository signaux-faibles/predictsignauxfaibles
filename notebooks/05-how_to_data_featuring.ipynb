{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "junior-booking",
   "metadata": {},
   "source": [
    "# How to test various featurings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitting-greek",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from predictsignauxfaibles.transformers import print_featuring_for_model_conf, get_featuring, apply_log, apply_sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-probe",
   "metadata": {},
   "source": [
    "# 1. List of features\n",
    "It is possible to create an array of variable names but also to use the file variables.json with the command as follows (by filling the `port`):\n",
    "\n",
    "```\n",
    "curl --proxy socks5h://localhost:<port> -OL https://raw.githubusercontent.com/signaux-faibles/opensignauxfaibles/master/js/reduce.algo2/docs/variables.json -o variables.json\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-offset",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output/data/variables.json\", 'r', encoding = 'utf-8') as f:\n",
    "    variables = json.load(f)\n",
    "\n",
    "features = list(set([x['name'] for x in variables]) - set(\"outcome\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-concert",
   "metadata": {},
   "source": [
    "# 2. Exploration of relevant featuring\n",
    "For each variable taken separately, the function `get_featuring` provide the optimal transformation for a single variable to explain the variable `outcome` in a simple LogisticModel. These transformations still need to be tested in the SF model afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-design",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = get_featuring(features, [apply_log, apply_sqrt])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "variable-relative",
   "metadata": {},
   "source": [
    "# 3. Print the tranformation for each feature if relevant ready to be plugged in a model_conf.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fuzzy-briefs",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_featuring_for_model_conf(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worst-liquid",
   "metadata": {},
   "source": [
    "# 4. (optional) Build/Export train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ready-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "from predictsignauxfaibles.utils import load_conf\n",
    "\n",
    "conf = load_conf(\"default\")\n",
    "\n",
    "train = conf.TRAIN_DATASET\n",
    "train.sample_size = 1e4\n",
    "\n",
    "test = conf.TEST_DATASET\n",
    "test.sample_size = 1e4\n",
    "\n",
    "savepath = \"output/data/featuring\"\n",
    "\n",
    "train.fetch_data().raise_if_empty()\n",
    "test.fetch_data().raise_if_empty()\n",
    "logging.info(\"Succesfully loaded Features data from MongoDB\")\n",
    "\n",
    "if savepath is not None:\n",
    "    train.data.to_csv(f\"{savepath}_train.csv\")\n",
    "    test.data.to_csv(f\"{savepath}_test.csv\")\n",
    "    logging.info(f\"Saved Features extract to {savepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-architect",
   "metadata": {},
   "source": [
    "# 5. Get data from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-treatment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from predictsignauxfaibles.config import IGNORE_NA\n",
    "from predictsignauxfaibles.pipelines import run_pipeline\n",
    "from predictsignauxfaibles.utils import load_conf\n",
    "from predictsignauxfaibles.evaluate import evaluate\n",
    "\n",
    "csvpath = \"output/data/featuring\"\n",
    "\n",
    "train_filepath = f\"{csvpath}_train.csv\"\n",
    "test_filepath = f\"{csvpath}_test.csv\"\n",
    "\n",
    "train_data = pd.read_csv(train_filepath)\n",
    "logging.info(f\"Succesfully loaded train data from {train_filepath}\")\n",
    "\n",
    "test_data = pd.read_csv(test_filepath)\n",
    "logging.info(f\"Succesfully loaded test data from {test_filepath}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern-printing",
   "metadata": {},
   "source": [
    "# 6. Evaluate a model with and without featuring and compare performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insured-expert",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_to_compare(train_data, test_data, conf_name:str = \"default\"):\n",
    "    conf = load_conf(conf_name)\n",
    "    train = conf.TRAIN_DATASET\n",
    "    train.sample_size = 1e4\n",
    "    \n",
    "    test = conf.TEST_DATASET\n",
    "    test.sample_size = 1e4\n",
    "    \n",
    "    train.data = train_data\n",
    "    test.data = test_data\n",
    "    \n",
    "    train_siren_set = train.data[\"siren\"].unique().tolist()\n",
    "    test.remove_siren(train_siren_set)\n",
    "    \n",
    "    train.replace_missing_data().remove_na(ignore=IGNORE_NA)\n",
    "    train.data = run_pipeline(train.data, conf.TRANSFO_PIPELINE)\n",
    "    \n",
    "    test.replace_missing_data().remove_na(ignore=IGNORE_NA)\n",
    "    test.data = run_pipeline(test.data, conf.TRANSFO_PIPELINE)\n",
    "    \n",
    "    model_pp = conf.MODEL_PIPELINE\n",
    "    fit = model_pp.fit(train.data, train.data[\"outcome\"])\n",
    "    \n",
    "    eval_metrics = evaluate(fit, test, conf.EVAL_BETA)\n",
    "    return {\n",
    "        'conf_name': conf_name,\n",
    "        'aucpr': eval_metrics['aucpr']   \n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordered-dallas",
   "metadata": {},
   "outputs": [],
   "source": [
    "perf_default = evaluate_to_compare(train_data, test_data, \"default\")\n",
    "perf_default_with_featuring = evaluate_to_compare(train_data, test_data, \"default_with_featuring\")\n",
    "print(perf_default)\n",
    "print(perf_default_with_featuring)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
