{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "from pymongo import MongoClient\n",
    "from pymongo.cursor import Cursor\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import pdb\n",
    "\n",
    "# Set logging level to INFO\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "from predictsignauxfaibles.utils import MongoDBQuery, MongoParams\n",
    "import predictsignauxfaibles.config as config\n",
    "import pytz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Métriques à mars 2020 (modèle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_features_from_file = True\n",
    "load_scores_from_file = True\n",
    "features_path = \"/home/simon.lebastard/predictsignauxfaibles/data/features_2103.json\"\n",
    "scores_path = \"/home/simon.lebastard/predictsignauxfaibles/data/scores_2103.json\"\n",
    "\n",
    "FEATURES_LIST = [\n",
    "    \"_id.siret\",\n",
    "    \"_id.periode\",\n",
    "    \"value.code_commune\",\n",
    "    \"value.departement\",\n",
    "    \"value.region\",\n",
    "    \"value.age\",\n",
    "    \"value.effectif\",\n",
    "    \"value.effectif_entreprise\",\n",
    "    \"value.time_til_failure\",\n",
    "    \"value.outcome\",\n",
    "    \"value.time_til_outcome\",\n",
    "    #insee\n",
    "    \"value.code_naf\",\n",
    "    \"value.libelle_naf\",\n",
    "    \"value.code_ape_niveau2\",\n",
    "    \"value.libelle_ape2\",\n",
    "    \"value.code_ape_niveau3\",\n",
    "    \"value.libelle_ape3\",\n",
    "    \"value.code_ape\",\n",
    "    # urssaf\n",
    "    \"value.montant_part_ouvriere\",\n",
    "    \"value.ratio_dette\",\n",
    "    \"value.cotisation\",\n",
    "    \"value.cotisation_moy12m\",\n",
    "    \"value.montant_part_ouvriere\",\n",
    "    \"value.montant_part_patronale\",\n",
    "    \"value.apart_heures_autorisees\",\n",
    "    \"value.apart_heures_consommees\",\n",
    "    \"value.apart_heures_consommees_cumulees\",\n",
    "    \"value.apart_entreprise\",\n",
    "    \"value.paydex_nb_jours\", #paydex\n",
    "    \"value.dette_fiscale\", #bdf\n",
    "    \"value.endettement\", #diane\n",
    "    \"value.taux_endettement\", #diane\n",
    "]\n",
    "\n",
    "LIBELLE_NAF = {\n",
    "    \"A\": \"Agriculture, sylviculture et pêche\",\n",
    "    \"B\": \"Industries extractives\",\n",
    "    \"C\": \"Industrie manufacturière\",\n",
    "    \"D\": \"Production et distribution d'électricité, de gaz, de vapeur et d'air conditionné\",\n",
    "    \"E\": \"Production et distribution d'eau ; assainissement, gestion des déchets et dépollution\",\n",
    "    \"F\": \"Construction\",\n",
    "    \"G\": \"Commerce ; réparation d'automobiles et de motocycles\",\n",
    "    \"H\": \"Transports et entreposage\",\n",
    "    \"I\": \"Hébergement et restauration\",\n",
    "    \"J\": \"Information et communication\",\n",
    "    \"K\": \"Activités financières et d'assurance\",\n",
    "    \"L\": \"Activités immobilières\",\n",
    "    \"M\": \"Activités spécialisées, scientifiques et techniques\",\n",
    "    \"N\": \"Activités de services administratifs et de soutien\",\n",
    "    \"O\": \"Administration publique\",\n",
    "    \"P\": \"Enseignement\",\n",
    "    \"Q\": \"Santé humaine et action sociale\",\n",
    "    \"R\": \"Arts, spectacles et activités récréatives\",\n",
    "    \"S\": \"Autres activités de services\",\n",
    "    \"T\": \"Activités des ménages en tant qu'employeurs ; activités indifférenciées des ménages en tant que producteurs de biens et services pour usage propre\",\n",
    "    \"U\": \"Activités extra-territoriales\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_datetime(date_str: str):\n",
    "    \"\"\"\n",
    "    Converts a string in the YYYY-MM-DD format into a\n",
    "    datetime\n",
    "    \"\"\"\n",
    "    return pytz.utc.localize(datetime.strptime(date_str, \"%Y-%m-%d\"))\n",
    "\n",
    "\n",
    "def datetime_to_str(dt: datetime):\n",
    "    \"\"\"\n",
    "    Converts a datetime.datetime object into its YYYY-MM-DD string representation\n",
    "    \"\"\"\n",
    "    return dt.strftime('%Y-%m-%d')\n",
    "\n",
    "def unravel_features(df: pd.DataFrame):\n",
    "    ids = df[\"_id\"].apply(pd.Series)\n",
    "    values = df[\"value\"].apply(pd.Series)\n",
    "    return pd.concat([ids, values], axis=1)\n",
    "\n",
    "\n",
    "def load_features_from_mongo(date_min: str, date_max: str, save_to_path: str = None):\n",
    "    features_mongo = MongoClient(host = \"mongodb://labbdd\").get_database(\"prod\").get_collection(\"Features\")\n",
    "    match_stage = {\n",
    "        \"$match\": {\n",
    "            \"$and\": [\n",
    "                {\n",
    "                \"_id.periode\": {\n",
    "                    \"$gte\": str_to_datetime(date_min),\n",
    "                    \"$lt\": str_to_datetime(date_max),\n",
    "                    },\n",
    "                \"_id.batch\": \"2103_0_urssaf\"\n",
    "                },\n",
    "                #{\"value.effectif\": {\"$gte\": min_effectif}},\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "\n",
    "    project_stage = {\n",
    "        \"$project\": {field: 1 for field in FEATURES_LIST}\n",
    "    }\n",
    "    pipeline = [match_stage, project_stage]\n",
    "\n",
    "    features_cursor = features_mongo.aggregate(pipeline)\n",
    "    logging.info(\"Loading features from MongoDB...\")\n",
    "    features_raveled = pd.DataFrame(features_cursor)\n",
    "    logging.info(\"... Success! Unravelling dataframe...\")\n",
    "    features = unravel_features(features_raveled)\n",
    "    features[\"siret\"] = features.siret.astype(int)\n",
    "    logging.info(\"...done!\")\n",
    "\n",
    "    if save_to_path is not None:\n",
    "        logging.info(\"Saving fetched Features data to disk\")\n",
    "        features_tosave = features.copy()\n",
    "        features_tosave.periode = features_tosave.periode.apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "        try:\n",
    "            features_tosave.to_json(save_to_path, orient=\"records\", default_handler=str)\n",
    "            logging.info(\"Success\")\n",
    "        except:\n",
    "            raise Exception(\"Features could not be saved to json\")\n",
    "        finally:\n",
    "            del features_tosave\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def load_features(date_min: str, date_max: str, from_file: bool = True, filepath: str = None):\n",
    "    if from_file:\n",
    "        if filepath is None:\n",
    "            raise Exception(\"Requesting to load features from file, but no filepath was provided\")\n",
    "        try:\n",
    "            features = pd.read_json(filepath, orient=\"records\")\n",
    "            logging.info(f\"Succesfully loaded Features data from {filepath}\")\n",
    "            features.periode = features.periode.apply(str_to_datetime)\n",
    "            return features\n",
    "        except FileNotFoundError:\n",
    "            logging.warning(f\"Filepath {filepath} was not found on disk. Fetching for MongoDB\")\n",
    "    \n",
    "    features = load_features_from_mongo(date_min, date_max, filepath)\n",
    "    return features\n",
    "        \n",
    "\n",
    "def load_scores_from_mongo(batch_name: str, algo_name: str, save_to_path: bool = True):\n",
    "    scores_mongo = MongoClient(host = \"mongodb://labbdd\").get_database(\"prod\").get_collection(\"Scores\")\n",
    "\n",
    "    match_stage = {\n",
    "        \"$match\": {\n",
    "            \"$and\": [\n",
    "                {\n",
    "                \"batch\": batch_name,\n",
    "                \"algo\": algo_name,\n",
    "                },\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    "    project_stage = {\n",
    "        \"$project\": {field: 1 for field in [\"siret\", \"batch\", \"algo\", \"periode\", \"alert\", \"score\", \"small_vs_final\"]}\n",
    "    }\n",
    "    pipeline = [match_stage, project_stage]\n",
    "\n",
    "    scores_cursor = scores_mongo.aggregate(pipeline)\n",
    "    logging.info(\"Loading collection Scores from MongoDB...\")\n",
    "    scores = pd.DataFrame(scores_cursor)\n",
    "    scores[\"periode\"] = scores.periode.dt.tz_localize(None)\n",
    "    scores.drop(columns=[\"_id\"], inplace=True)\n",
    "\n",
    "    if save_to_path is not None:\n",
    "        logging.info(\"Saving fetched Scores data to disk\")\n",
    "        try:\n",
    "            scores.to_json(save_to_path, orient=\"records\", default_handler=str)\n",
    "            logging.info(\"Success\")\n",
    "        except:\n",
    "            raise Exception(\"Fetch of Scores could not be saved to json\")\n",
    "    \n",
    "    return scores\n",
    "\n",
    "\n",
    "def load_scores(batch_name: str, algo_name: str, from_file: bool = True, filepath: str = None):\n",
    "    if from_file:\n",
    "        if filepath is None:\n",
    "            raise Exception(\"Requesting to load scores from file, but no filepath was provided\")\n",
    "        try:\n",
    "            scores = pd.read_json(filepath, orient=\"records\")\n",
    "            logging.info(f\"Succesfully loaded Scores data from {filepath}\")\n",
    "            scores.periode = scores.periode.apply(str_to_datetime)\n",
    "            return scores\n",
    "        except FileNotFoundError:\n",
    "            logging.warning(f\"Filepath {filepath} was not found on disk. Fetching for MongoDB\")\n",
    "    \n",
    "    scores = load_scores_from_mongo(batch_name, algo_name, filepath)\n",
    "    #scores.periode = scores.periode.apply(str_to_datetime)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching features and scores data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = load_features(date_min=\"2020-02-01\", date_max=\"2020-02-28\", from_file=False, filepath=features_path)\n",
    "logging.info(f\"Loaded {features.shape[0]} rows and {features.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = load_scores(batch_name=\"2102_altares\", algo_name=\"mars2021_v0\", from_file=True, filepath=scores_path)\n",
    "logging.info(f\"Loaded {scores.shape[0]} rows and {scores.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[\"periode\"] = features.periode.apply(datetime_to_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[\"periode\"] = scores.periode.apply(datetime_to_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(scores, features, on=['siret', 'periode'], how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "postproc_path = \"/home/simon.lebastard/predictsignauxfaibles/data/postproc_2103.json\"\n",
    "if not os.path.isfile(postproc_path):\n",
    "    logging.info(\"Saving joined post-processed data to disk...\")\n",
    "    df.to_json(postproc_path, orient=\"records\", default_handler=str)\n",
    "    logging.info(f\"Saved to {postproc_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, load data directly from df stored on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(postproc_path):\n",
    "    print(\"Loading post-processed data to disk\")\n",
    "    df = pd.read_json(postproc_path, orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation of region-wide features\n",
    "\n",
    "Niveaux de granularité considérés:\n",
    "- région\n",
    "- département\n",
    "\n",
    "Pour chaque niveau de granularité:\n",
    "- compter le nombre d'entreprises flaguées rouge par région\n",
    "- compter le nombre d'entreprises flaguées orange par région\n",
    "- calculer les ratio correspondants\n",
    "- [BONUS: A NE PAS COMMUNIQUER] produire un score de risque moyen par région\n",
    "- compter le nombre de défaillances effectives sur une période donnée, et calculer le ratio correspondants\n",
    "\n",
    "Pour toutes les grandeurs calculées précédemment, calculer des équivalents en nombre d'employés concernés\n",
    "\n",
    "En bonus: calculer des indicateurs de contribution des chaques 4 macrovariables du modèle\n",
    "\n",
    "### Preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dct_nan_rpl = {\n",
    "    \"montant_part_ouvriere\": 0.0,\n",
    "    \"montant_part_patronale\": 0.0,\n",
    "    \"apart_heures_autorisees\": 0.0,\n",
    "    \"apart_heures_consommees\": 0.0,\n",
    "    \"apart_heures_consommees_cumulees\": 0.0,\n",
    "    \"dette_fiscale\": 0.0,\n",
    "    \"endettement\": 0.0,\n",
    "    \"taux_endettement\": 0.0\n",
    "}\n",
    "\n",
    "def preprocess(df):\n",
    "    # create an outcome flag based only on failures since the beginning of the COVID crisis\n",
    "    df[\"failure\"] = (df[\"time_til_failure\"]>=0) & (df[\"time_til_failure\"]<12) # todo: automatiser le nombre de mois à regarder vers l'avant: entre mars 2020 et <THIS_MONTH>\n",
    "    df[\"failure\"] = df.failure.astype(int)\n",
    "\n",
    "    # encode alert level into integer\n",
    "    df[\"alert_flag\"] = df.alert.replace({\"Pas d'alerte\": 0, \"Alerte seuil F1\": 1, \"Alerte seuil F2\": 2})\n",
    "    df[\"alert_bin\"] = (df.alert_flag > 0)\n",
    "\n",
    "    # ratio dette/cotisation sur la part salariale des cotisations sociales\n",
    "    df[\"ratio_dette_ouvriere\"] = df[\"montant_part_ouvriere\"] / df[\"cotisation\"]\n",
    "    df[\"ratio_dette_patronale\"] = df[\"montant_part_patronale\"] / df[\"cotisation\"]\n",
    "    return df\n",
    "\n",
    "def replace_nans(df, replace_dct):\n",
    "    for field, rpl in replace_dct.items():\n",
    "        df[field].fillna(value=rpl, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_nans(df, dct_nan_rpl)\n",
    "df = preprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(by=[\"failure\"]).siret.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(by=[\"alert_flag\"]).siret.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(by=[\"alert_bin\"]).siret.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building aggregation dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_stats(geo_attr, outcome_attr):\n",
    "    assert outcome_attr in [\"alert_flag\", \"alert_bin\", \"failure\", \"outcome\"]\n",
    "\n",
    "    risk_ape3_stats = df.groupby(by=[geo_attr,outcome_attr,\"libelle_naf\",\"libelle_ape3\"]).agg(\n",
    "        siret_count=('siret', 'count'),\n",
    "        effectif_tot=('effectif', 'sum'),\n",
    "    )\n",
    "    risk_naf_stats = df.groupby(by=[geo_attr,outcome_attr,\"libelle_naf\"]).agg(\n",
    "        siret_count=('siret', 'count'),\n",
    "        effectif_tot=('effectif', 'sum')\n",
    "    )\n",
    "    risk_stats = df.groupby(by=[geo_attr,outcome_attr]).agg(\n",
    "        siret_count=('siret', 'count'),\n",
    "        effectif_tot=('effectif', 'sum'),\n",
    "        ratiodette_ouvr_avg=('ratio_dette_ouvriere', 'mean'),\n",
    "        ratiodette_patr_avg=('ratio_dette_patronale', 'mean'),\n",
    "        ratiodette_avg=('ratio_dette', 'mean'),\n",
    "        apart_autr_avg=('apart_heures_autorisees', 'mean'),\n",
    "        apart_cons_avg=('apart_heures_consommees', 'mean'),\n",
    "        apart_cumcons_avg=('apart_heures_consommees_cumulees', 'mean'),\n",
    "        paydex_avg=('paydex_nb_jours', 'mean'),\n",
    "        taux_endettement_avg=('taux_endettement', 'mean'),\n",
    "    )\n",
    "    national_stats = df.groupby(by=[outcome_attr]).agg(\n",
    "        siret_count=('siret', 'count'),\n",
    "        effectif_tot=('effectif', 'sum'),\n",
    "        ratiodette_ouvr_avg=('ratio_dette_ouvriere', 'mean'),\n",
    "        ratiodette_patr_avg=('ratio_dette_patronale', 'mean'),\n",
    "        ratiodette_avg=('ratio_dette', 'mean'),\n",
    "        apart_autr_avg=('apart_heures_autorisees', 'mean'),\n",
    "        apart_cons_avg=('apart_heures_consommees', 'mean'),\n",
    "        apart_cumcons_avg=('apart_heures_consommees_cumulees', 'mean'),\n",
    "        paydex_avg=('paydex_nb_jours', 'mean'),\n",
    "        taux_endettement_avg=('taux_endettement', 'mean'),\n",
    "    )\n",
    "\n",
    "    risk_stats['siret_rate'] = 100*risk_stats.siret_count / risk_stats.groupby(by=geo_attr).siret_count.sum()\n",
    "    risk_stats['effectif_rate'] = 100*risk_stats.effectif_tot / risk_stats.groupby(by=geo_attr).effectif_tot.sum()\n",
    "\n",
    "    risk_stats['ape3_mostatrisk_eff'] = risk_ape3_stats.loc[risk_ape3_stats.index.get_level_values(\"libelle_naf\").isin([LIBELLE_NAF[code] for code in [\"B\", \"C\", \"D\", \"E\"]])].groupby(by=[geo_attr,outcome_attr]).effectif_tot.idxmax()\n",
    "    risk_stats['ape3_mostatrisk_eff'] = risk_stats.ape3_mostatrisk_eff.apply(lambda x: x[3] if isinstance(x, tuple) else \"None\")\n",
    "    risk_stats['ape3_risk_eff'] = risk_ape3_stats.loc[risk_ape3_stats.index.get_level_values(\"libelle_naf\").isin([LIBELLE_NAF[code] for code in [\"B\", \"C\", \"D\", \"E\"]])].groupby(by=[geo_attr,outcome_attr]).effectif_tot.max()\n",
    "\n",
    "    risk_stats['naf_mostatrisk_eff'] = risk_naf_stats.groupby(by=[geo_attr,outcome_attr]).effectif_tot.idxmax()\n",
    "    risk_stats['naf_mostatrisk_eff'] = risk_stats.naf_mostatrisk_eff.apply(lambda x: x[2] if isinstance(x, tuple) else \"None\")\n",
    "    risk_stats['naf_risk_eff'] = risk_naf_stats.groupby(by=[geo_attr,outcome_attr]).effectif_tot.max()\n",
    "\n",
    "    risk_stats['ape3_mostatrisk_etab'] = risk_ape3_stats.loc[risk_ape3_stats.index.get_level_values(\"libelle_naf\").isin([LIBELLE_NAF[code] for code in [\"B\", \"C\", \"D\", \"E\"]])].groupby(by=[geo_attr,outcome_attr]).siret_count.idxmax()\n",
    "    risk_stats['ape3_mostatrisk_etab'] = risk_stats.ape3_mostatrisk_etab.apply(lambda x: x[3] if isinstance(x, tuple) else \"None\")\n",
    "    risk_stats['ape3_risk_etab'] = risk_ape3_stats.loc[risk_ape3_stats.index.get_level_values(\"libelle_naf\").isin([LIBELLE_NAF[code] for code in [\"B\", \"C\", \"D\", \"E\"]])].groupby(by=[geo_attr,outcome_attr]).siret_count.max()\n",
    "\n",
    "    risk_stats['naf_mostatrisk_etab'] = risk_naf_stats.groupby(by=[geo_attr,outcome_attr]).siret_count.idxmax()\n",
    "    risk_stats['naf_mostatrisk_etab'] = risk_stats.naf_mostatrisk_etab.apply(lambda x: x[2] if isinstance(x, tuple) else \"None\")\n",
    "    risk_stats['naf_risk_etab'] = risk_naf_stats.groupby(by=[geo_attr,outcome_attr]).siret_count.max()\n",
    "\n",
    "    national_stats['siret_rate'] = 100*national_stats.siret_count / national_stats.siret_count.sum()\n",
    "    national_stats['effectif_rate'] = 100*national_stats.effectif_tot / national_stats.effectif_tot.sum()\n",
    "\n",
    "\n",
    "    risk_stats['siret_rate_to_ntl_avg'] = 100*(risk_stats.siret_rate - national_stats.siret_rate) / national_stats.siret_rate\n",
    "    risk_stats['effectif_rate_to_ntl_avg'] = 100*(risk_stats.effectif_rate - national_stats.effectif_rate) / national_stats.effectif_rate\n",
    "    risk_stats['ratiodette_ouvr_avg'] = 100*(risk_stats.ratiodette_ouvr_avg - national_stats.ratiodette_ouvr_avg) / national_stats.ratiodette_ouvr_avg\n",
    "    risk_stats['ratiodette_patr_avg'] = 100*(risk_stats.ratiodette_patr_avg - national_stats.ratiodette_patr_avg) / national_stats.ratiodette_patr_avg\n",
    "    risk_stats['ratiodette_avg'] = 100*(risk_stats.ratiodette_avg - national_stats.ratiodette_avg) / national_stats.ratiodette_avg\n",
    "    risk_stats['apart_autr_avg'] = 100*(risk_stats.apart_autr_avg - national_stats.apart_autr_avg) / national_stats.apart_autr_avg\n",
    "    risk_stats['apart_cons_avg_to_ntl_avg'] = 100*(risk_stats.apart_cons_avg - national_stats.apart_cons_avg) / national_stats.apart_cons_avg\n",
    "    risk_stats['apart_cumcons_avg_to_ntl_avg'] = 100*(risk_stats.apart_cumcons_avg - national_stats.apart_cumcons_avg) / national_stats.apart_cumcons_avg\n",
    "    risk_stats['paydex_avg_to_ntl_avg'] = 100*(risk_stats.paydex_avg - national_stats.paydex_avg) / national_stats.paydex_avg\n",
    "    risk_stats['taux_endettement_avg'] = 100*(risk_stats.taux_endettement_avg - national_stats.taux_endettement_avg) / national_stats.taux_endettement_avg\n",
    "\n",
    "    return risk_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_risk_stats = aggregate_stats(geo_attr=\"region\", outcome_attr=\"alert_flag\")\n",
    "reg_riskbin_stats = aggregate_stats(geo_attr=\"region\", outcome_attr=\"alert_bin\")\n",
    "reg_fail_stats = aggregate_stats(geo_attr=\"region\", outcome_attr=\"failure\")\n",
    "\n",
    "dpt_risk_stats = aggregate_stats(geo_attr=\"departement\", outcome_attr=\"alert_flag\")\n",
    "dpt_riskbin_stats = aggregate_stats(geo_attr=\"departement\", outcome_attr=\"alert_bin\")\n",
    "dpt_fail_stats = aggregate_stats(geo_attr=\"departement\", outcome_attr=\"failure\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_risk_outpath_root = \"/home/simon.lebastard/predictsignauxfaibles/data/reg_2103\"\n",
    "dpt_risk_outpath_root = \"/home/simon.lebastard/predictsignauxfaibles/data/dpt_2103\"\n",
    "\n",
    "reg_risk_stats.to_csv(f\"{reg_risk_outpath_root}_riskflag.csv\")\n",
    "dpt_risk_stats.to_csv(f\"{dpt_risk_outpath_root}_riskflag.csv\")\n",
    "reg_riskbin_stats.to_csv(f\"{reg_risk_outpath_root}_riskbin.csv\")\n",
    "dpt_riskbin_stats.to_csv(f\"{dpt_risk_outpath_root}_riskbin.csv\")\n",
    "reg_fail_stats.to_csv(f\"{reg_risk_outpath_root}_failures.csv\")\n",
    "dpt_fail_stats.to_csv(f\"{dpt_risk_outpath_root}_failures.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 64-bit ('env_predictSF')",
   "name": "python368jvsc74a57bd0619141f40d6660e90435fdea452ff8c8a3cfa2ce2d42e30ecc199b88a9b310ca"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
